\subsection{Related Work}\label{sec:related}

There is a large amount of literature on Gaussian Processes and spatiotemporal modeling, a complete survey of which is beyond the scope of this paper. Since our contributions are in the area of creating a feedback-based observer in the feature spaces of GP models, we will here discuss  related work in three related areas: spatiotemporal modeling with GPs, the connection of GPs to Kalman filtering, and sensor placement for inference in spatiotemporal domains.
 
The use of process-dependent kernels for spatiotemporal modeling in geostatistics is well-studied \cite{wikle2002kernel,cressie2011statistics,stroud2001dynamic}. Other approaches that utilize hierarchy or evolution of kernels have also been used for modeling spatiotemporal functions \cite{hartikainen2013sequential,lindgren2011explicit,ho1996multiresolution}.
From the machine learning perspective, a naive approach is to utilize both spatial and temporal variables as inputs to a Mercer kernel \cite{perez2013gaussian}. However, this technique leads to an ever-growing kernel dictionary. %, which is computationally taxing.
Furthermore, constraining the dictionary size or utilizing a moving window will occlude learning of long-term patterns. A more clever approach is to use state-space representations of time-varying GPs \cite{sarkka2014convergence,hartikainen2013sequential}. From this viewpoint, each GP instance is viewed as a snapshot of an evolving set of weights. We follow in a similar vein here, with added emphasis on exploiting mathematical structures relevant to observability and controllability. Periodic or nonstationary covariance functions and nonlinear transformations have been proposed for spatiotemporal modeling \cite{ma2003nonstationary,RasmussenWilliams2005}. Work focusing on nonseparable and nonstationary covariance kernels seeks to design kernels optimized for environment-specific dynamics, and to tune their hyperparameters in local regions of the input space. Seminal work in \cite{higdon1998process} proposes a process convolution approach for space-time modeling. This model captures nonstationary structure by allowing the convolution kernel to vary across the input space. This approach can be extended to a class of nonstationary covariance functions, thereby allowing the use of a Gaussian process (GP) framework, as shown in \cite{paciorek2004nonstationary}. However,  since this model's hyperparameters are inferred using MCMC integration, its application has been limited to smaller datasets. To overcome this limitation, \cite{plagemann2008nonstationary} proposes to use the mean estimates of a second isotropic GP (defined over latent length scales) to parameterize the nonstationary covariances. Finally, \cite{garg2012AAAI} considers non-isotropic variation across different dimensions of input space for the second GP as opposed to isotropic variation by \cite{plagemann2008nonstationary}. Issues with this line of approach include the nonconvexity of the hyperparameter optimization problem and the fact that selection of an appropriate nonstationary covariance function for the task at hand is a nontrivial design decision (as noted in \cite{singh2010modeling}). 

Apart from directly modeling the covariance function using additional latent GPs, there exist several other approaches for specifying nonstationary GP models. One approach maps the nonstationary spatial process into a latent space, in which the problem becomes approximately stationary \cite{schmidt2003bayesian}. Along similar lines, \cite{pfingsten2006nonstationary} extends the input space by adding latent variables, which allows the model to capture  nonstationarity in original space. Both these approaches require MCMC sampling for inference, and as such are subject to the limitations mentioned in the preceding paragraph. 
A geostatistics approach that finds dynamical transition models on the linear combination of weights of a parameterized model \cite{cressie2011statistics,mardia1998kriged} is advantageous when the spatial and temporal dynamics are hierarchically separated, leading to a convex learning problem. This approach has been utilized previously in MRI imaging \cite{noh2007testing,noh2012space}. As a result, complex nonstationary kernels are often not necessary (although they can be accommodated). This approach is essentially the starting point of this tutorial. 
A systems-theoretic study of this viewpoint enables our fundamental contributions, which are 1) allowing for inference on more general domains with a larger class of basis functions than those typically considered in the geostatistics community, and 2) quantifying the minimum number of measurements required to estimate the state of the system. 

Kalman filtering in the context of Gaussian processes and kernel models has also been quite widely studied \cite{carron2016machine,hartikainen2010kalman,sarkka2013spatiotemporal,stroud2001dynamic,miller1986toward}.  There is a direct link between the Bayesian approach to inference taken in GPs and its natural extension to Kalman Filters. Our contributions here are in creating explicit connections between feedback observers and inference by deriving conditions on observability in the kernel space. This leads to explicit conditions on the number of sensors required and where to place them. Lastly, sensor placement optimization is also a well-studied area. Examples include, but are not limited to 1) geometric approaches, which seek to provide a covering of the operating space without making assumptions about the spatiotemporal dynamics \cite{egerstedt:bk:2010}, and 2) information-theoretic approaches, which place their focus on sensor placement optimizing strategies based on mutual information and information entropy for Gaussian process models \cite{Guestrin05_ICML}. It should be noted that the contribution of our work concerning sensor placement is to provide \emph{sufficient conditions} for monitoring rather than optimization of the placement locations, and therefore a comparison with these approaches is not considered in the experiments. 

Lastly, we connect our work to the large body of literature produced in the last decade on Koopman operator theory and Dynamic Mode Decomposition, particularly in the Computational Fluid Dynamics (CFD) community. These methods rely on discovering \emph{modes} of motion which show the spatial distribution, oscillation frequency, and growth rate/decay of the component dynamics of the system. Many applications have been realized through these methods, which include the ability to transform the state space so the dynamics appear linear, to predict the temporal evolution of the linear system, to reconstruct the state of the original nonlinear system, and even to implement controller design. Dynamic Mode Decomposition (DMD) is the most widely used method for finding a finite-dimensional subspace of the Koopman operator's infinite-dimensional domain to work in \cite{schmid2010dynamic}. Williams et al., recently integrated DMD with the kernel trick, allowing the algorithm to be extended to systems with much larger dimensions \cite{williams2015kerneldmd}. Brunton et al., inspired by DMD, were able to generate governing equations from data by sparse identification of nonlinear dynamical systems \cite{brunton2016discovering}. However, these methods are restricted to approximating the Koopman operator given a fixed vector-valued observable, and have no way of effectively using measurements that vary in both number and location over time. Furthermore, the state of research into data-driven generalizing over similar systems with varying parameters is at best preliminary.
