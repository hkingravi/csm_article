\vspace{-0.1in}
\section{Kernel Observers}\label{sec:observers}

This section outlines our modeling framework and presents theoretical results associated with the number of sampling locations required for monitoring functional evolution. 
\input{sec_preliminaries}
\vspace{-0.1in}




\subsection{Main Results}\label{sec:theory_results}
%\vspace{-0.1in}
In this section, we prove results concerning the observability of spatiotemporally varying functions modeled by the functional evolution and measurement equations \eqref{k_measure}. In particular,  observability of the system states implies that we can recover the current state of the spatiotemporally varying function using a small number of sampling locations $\nsamp$, which allows us to 1) track the function, and 2) predict its evolution forward in time. We work with the approximation $\fspaceApprox\approx\fspace$: given $\ncent$ basis functions, this implies that the dual space of $\fspaceApprox$ is $\R^{\ncent}$.
Proposition \ref{prop:1} shows that if $\dualopApprox$ has a full-rank Jordan decomposition, the observation matrix $\obsMat$ meeting a condition called \emph{shadedness} (Definition \ref{def:shaded}) is sufficient for the system to be observable. Proposition \ref{prop:2} provides a lower bound on the number of sampling locations required for observability which holds for any $\dualopApprox$.  Proposition \ref{prop:3} constructively shows the existence of an abstract measurement map $\measmap$ achieving this lower bound. Since the measurement map does not have the structure of a kernel matrix, a slightly weaker sufficient condition for the observability of any $\dualopApprox$ is in Theorem \ref{thm:1}. Finally, since both $\empK$ and $\empKCD$ are kernel matrices generated from a shared kernel, these observability results translate directly into controllability results. %Proofs of all claims are in the appendix. 

\begin{definition}\label{def:shaded}
\textbf{(Shaded Observation Matrix)} Given $\kernel:\dom\times\dom\to\R$ positive-definite on a domain $\dom$, let $\{\fmapApprox_1(x), \dots, \fmapApprox_{\ncent}(x)\}$ be the set of bases generating an approximate feature map $\fmapApprox:\dom\to\fspaceApprox$, and let
$\sampSet = \sampSetLong$ be the set of sampling (or sensing) locations, with each $x_i\in\dom$. 
Let $\obsMat\in\R^{\nsamp\times\ncent}$ be the observation matrix, where $ \obsMat_{ij} := \fmapApprox_j(x_i)$. For each
row $\obsMat_{(i)} := \left[\begin{smallmatrix}
  \fmapApprox_1(x_i) & \cdots & \fmapApprox_{\ncent}(x_i)
 \end{smallmatrix}\right]$, define the set 
$\Ind_{(i)} := \{\iota_1^{(i)},\iota_2^{(i)},\dots, \iota_{\ncent_i}^{(i)}\}$ to be the indices in the observation
matrix row $i$ which are nonzero. 
Then if 
%\begin{align}\eqlabel{shaded_cond}
$\bigcup_{i\in\{1,\dots,\nsamp\}} \Ind^{(i)} = \{1,2,\dots, \ncent\}$,
%\end{align}
we denote $\obsMat$ as a \emph{shaded observation matrix} (see Figure \ref{fig:shadeda}).
\end{definition}

This definition seems quite abstract, so the following remark considers a more concrete example.

\begin{remark}\label{rem:shaded}
 let $\fmapApprox$ be generated by the dictionary given by $\shCent = \shCentLong$, $c_i\in\dom$. Note that since $\fmapApprox_j(x_i) = \l\fmap(x_i), \fmap(c_j)\r_{\fspace} = \kernel(x_i,c_j)$, $\obsMat$ is the kernel matrix between $\sampSet$ and $\shCent$. For the kernel matrix to be shaded thus implies that there does not exist an atom $\fmap(c_j)$ such that the projections $\l\fmap(x_i),\fmap(c_j)\r_{\fspace}$ vanish for all $x_i$, $1\leq i\leq \nsamp$. Intuitively, the shadedness property requires that the sensor locations $x_i$ are privy to information propagating from every $c_j$. As an example, note that, in principle, for the Gaussian kernel, a single row generates a shaded kernel matrix. However, in this case, the matrix can have many entries that are extremely close to zero, and will probably be very ill-conditioned.  
\end{remark}

% With this definition in place, we can prove the following proposition, which shows that if $\dualopC$ has a full-rank Jordan decomposition, a shaded observationrf matrix is sufficient to guarantee observability. 

\begin{proposition}\label{prop:1}
Given $\kernel:\dom\times\dom\to\R$ positive-definite on a domain $\dom$, let $\{\fmapApprox_1(x), \dots, \fmapApprox_{\ncent}(x)\}$ be the set of bases generating an approximate feature map $\fmapApprox:\dom\to\fspaceApprox$, and let
$\sampSet = \sampSetLong$, $x_i\in\dom$. Consider the discrete linear system on $\fspaceApprox$ given by the evolution and measurement equations \eqref{k_measure}. Suppose that a full-rank Jordan decomposition of $\dualopApprox\in\R^{\ncent\times\ncent}$ of the form $\dualopApprox = \JorP\JorLa\JorP^{-1}$ exists, where $\JorLa = 
\left[\begin{smallmatrix}\JorLa_1 &\cdots & \JorLa_{\JorMul}\end{smallmatrix}\right]$,
and there are no repeated eigenvalues. Then, given a set of time instances  $\Tset = \{\tindex_1,\tindex_2,\dots,\tindex_{\otime}\}$, and a set of sampling locations $\sampSet=\sampSetLong$,
the system \eqref{k_measure} is observable if the observation matrix $\empK_{ij}$ is shaded according to Definition \ref{def:shaded},
% $\empK^D$, the row vector generated by summing the rows of $\empK$, has all nonzero entries, 
$\Tset$ has distinct values, and $|\Tset| \geq \ncent$.
\end{proposition}

\begin{proof}
	To begin, consider a system where $\dualopApprox = \JorLa$, with Jordan blocks $\{\JorLa_1, \JorLa_2, \dots, \JorLa_{\JorMul}\}$ along the 
	diagonal. Then $\dualopApprox^{\tindex_i} = \diag(\begin{bmatrix}\JorLa_1^{\tindex_i} & \JorLa_2^{\tindex_i} & \cdots & \JorLa_{\JorMul}^{\tindex_i}\end{bmatrix})$. 
	%The exponentiation of each Jordan block results in upper triangular matrices with linearly independent columns. 
	We have that:
	\begin{align*}
	\Obs_{\Tset} &=
	\underbrace{
		\begin{bmatrix} 
		\empK \dualopApprox^{\tindex_1}\\
		\cdots\\
		\empK \dualopApprox^{\tindex_\otime}.
		\end{bmatrix}}_{\Obs_{\Tset}\in\R^{\nsamp\otime\times\ncent}}
	\end{align*}
	We need to prove that the column rank of $\Obs_{\Tset}$ is $\ncent$, which is not immediately
	obvious since typically $\nsamp \ll \ncent$. To prove the statement, we will show that 
	computing the rank of $\Obs_{\Tset}$ is equivalent to the rank computation of the product of 
	two simple matrices. In what follows,
	we use the notation $\zerosMat{I}{J}$ to denote an $I\times J$ matrix of all zeros. 
	
	In the first step, we write the above matrix as the product of two matrices. Then it can be	shown that $\Obs_{\Tset}$ is the product of two block matrices.
	\begin{align*}
	\Obs_{\Tset}  
	&=
	\underbrace{
		\begin{bmatrix}
		\empK   & \cdots & \zerosK\\
		\vdots  & \ddots & \vdots\\
		\zerosK  & \cdots & \empK
		\end{bmatrix}}_{\catempK\in\R^{\nsamp\otime\times\ncent\otime}}
	\underbrace{
		\begin{bmatrix}
		\JorLa_1^{\tindex_1} & \cdots & 0\\
		\vdots & \ddots & \vdots\\
		0 & \cdots & \JorLa_{\JorMul}^{\tindex_1}\\ 
		\hline
		\vdots & \ddots & \vdots\\
		\hline
		\JorLa_1^{\tindex_{\otime}} & \cdots & 0\\
		\vdots & \ddots & \vdots\\
		0 & \cdots & \JorLa_{\JorMul}^{\tindex_\otime}   
		\end{bmatrix}}_{\catdualopC\in\R^{\ncent\otime\times\ncent}}.
	\end{align*}
	We need to simplify $\catempK$ even further. 
	Recall that a matrix's rank is preserved under a product with an invertible matrix. Design a 
	matrix of elementary row operations $U\in\R^{\nsamp\times\nsamp}$ such that $\transempK := U\empK$ is a matrix with at least one row vector of nonzeros; this can be achieved by having an elementary matrix that adds rows together. By the shadedness assumption, such a matrix exists. We can write this operation as:
	\begin{align*}
	U\empK = 
	\begin{bmatrix}
	\transempK_{11} & \transempK_{12} & \cdots & \transempK_{1\ncent}\\
	\transempK_{21} & \transempK_{22} & \cdots & \transempK_{2\ncent}\\
	\vdots & \vdots & \ddots & \vdots\\
	\transempK_{\nsamp1} & \transempK_{\nsamp2} & \cdots & \transempK_{\nsamp\ncent}.
	\end{bmatrix}
	\end{align*}
	Without loss of generality, and abusing notation slightly, let this multiplication lead to one nonzero row, with the rest of the elements of the matrix being zero, as:
	\begin{align*}
	U\empK = 
	\begin{bmatrix}
	\kernel_{11} & \kernel_{12} & \cdots & \kernel_{1\ncent}\\
	0 & 0 & \cdots & 0\\
	\vdots & \vdots & \ddots & \vdots\\
	0 & 0 & \cdots & 0
	\end{bmatrix}.
	\end{align*}
	Since elementary matrices are full-rank, we then have that $\Rank(U\empK) = \Rank(\empK)$. 
	
	To analyze the rank of $\Obs_{\Tset}$, we apply these elementary matrices to every $\empK\in\catempK$. To do so, consider the block-diagonal matrix $\bdU\in\R^{\nsamp\otime\times\nsamp\otime}$ with $U\in\R^{\nsamp\times\nsamp}$ along the diagonal, and zeros everywhere else,
	i.e.:
	\begin{align}
	\bdU:= \begin{bmatrix}
	U   & \zerosU & \cdots & \zerosU\\
	\zerosU & U   & \cdots & \zerosU\\
	\vdots  & \vdots  & \ddots & \vdots\\
	\zerosU & \zerosU & \cdots & U
	\end{bmatrix}.
	\end{align}
	It can be shown that  $\bdU$ is full-rank, i.e. has rank $\nsamp\otime$. Going back to the observability matrix, we have that:
	\begin{align*}
	\bdU\Obs_{\Tset} &= \bdU\catempK\catdualopC\\
	%  \underbrace{
	%  \begin{bmatrix}
	%   U   & \zerosU & \cdots & \zerosU\\
	%   \zerosU & U   & \cdots & \zerosU\\
	%   \vdots  & \vdots  & \ddots & \vdots\\
	%   \zerosU & \zerosU & \cdots & U
	%  \end{bmatrix}}_{\bdU\in\R^{\nsamp\otime\times\nsamp\otime}}
	%  \underbrace{
	%  \begin{bmatrix}
	%   \empK   & \zerosK & \cdots & \zerosK\\
	%   \zerosK & \empK   & \cdots & \zerosK\\
	%   \vdots  & \vdots  & \ddots & \vdots\\
	%   \zerosK & \zerosK & \cdots & \empK
	%  \end{bmatrix}}_{\catempK\in\R^{\nsamp\otime\times\ncent\otime}}
	%  \underbrace{
	%  \begin{bmatrix}
	%   \JorLa_1^{\tindex_1} & 0 & \cdots & 0\\
	%   0 & \JorLa_2^{\tindex_1} & \cdots & 0\\
	%    \vdots & \vdots & \ddots & \vdots\\
	%   0 & 0 & \cdots & \JorLa_{\JorMul}^{\tindex_1}\\ 
	%    \hline
	%    \vdots & \vdots & \ddots & \vdots\\
	%    \hline
	%   \JorLa_1^{\tindex_{\otime}} & 0 & \cdots & 0\\
	%   0 & \JorLa_2^{\tindex_\otime} & \cdots & 0\\
	%    \vdots & \vdots & \ddots & \vdots\\
	%   0 & 0 & \cdots & \JorLa_{\JorMul}^{\tindex_\otime}   
	%  \end{bmatrix}}_{\catdualopC\in\R^{\ncent\otime\times\ncent}}\\
	&=  
	\underbrace{
		\begin{bmatrix}
		U\empK   & \zerosK & \cdots & \zerosK\\
		\zerosK & U\empK   & \cdots & \zerosK\\
		\vdots  & \vdots  & \ddots & \vdots\\
		\zerosK & \zerosK & \cdots & U\empK
		\end{bmatrix}}_{\bdU\catempK\in\R^{\nsamp\otime\times\ncent\otime}}
	\underbrace{\catdualopC}_{\in\R^{\ncent\otime\times\ncent}},
	\end{align*}
	since $\zerosU\zerosK = \zerosK$. Due to the fact that $\Rank(\bdU\Obs_{\Tset}) = \Rank(\Obs_{\Tset})$,
	we can therefore perform our rank analysis on the simpler matrix $\Rank(\bdU\Obs_{\Tset})$. Note that:
	\begin{align*}
	U\empK \dualopApprox^{\tindex_j} &=  
	\begin{bmatrix}
	\kernel_{11} & \kernel_{12} & \cdots & \kernel_{1\ncent}\\
	0 & 0 & \cdots & 0\\
	\vdots & \vdots & \ddots & \vdots\\
	0 & 0 & \cdots & 0
	\end{bmatrix}
	\dualopApprox^{\tindex_j}\\
	&= 
	\begin{bmatrix}
	k_{11}\la_1^{\tindex_j} & \binom{\tindex_j}{1}\la_1^{\tindex_j-1} + k_{12}\la_1^{\tindex_j}  & \cdots & k_{1\ncent}\la_{\JorMul}^{\tindex_j}\\
	0 & 0 & \cdots & 0\\
	\vdots & \vdots & \ddots & 0\\
	0 & 0 & \cdots & 0
	\end{bmatrix}.  
	\end{align*}
	Therefore, following some more elementary row operations encoded by $V\in\R^{\ncent\otime\times\ncent\otime}$, we have that:
	\begin{align*}
	V \bdU\Obs_{\Tset}
	&= 
	\begin{bmatrix}
	\kernel_{11}\la_1^{\tindex_1} & \cdots & \kernel_{1\ncent}\la_{\JorMul}^{\tindex_1}\\
	\kernel_{11}\la_1^{\tindex_2} & \cdots & \kernel_{1\ncent}\la_{\JorMul}^{\tindex_2}\\
	\vdots & \ddots & 0\\
	\kernel_{11}\la_1^{\tindex_{\otime}} & \cdots 
	& \kernel_{1\ncent}\la_{\JorMul}^{\tindex_{\otime}}\\
	\zerosMat{\ncent(\otime-1)}{1} & \cdots & \zerosMat{\ncent(\otime-1)}{1}
	\end{bmatrix}\\
	&= 
	\begin{bmatrix}
	\boldsymbol{\Phi}\\
	\zerosMat{\ncent(\otime-1)}{\ncent}
	\end{bmatrix}.
	\end{align*}
	If the individual entries $\kernel_{1i}$ are nonzero, and the Jordan block diagonals have nonzero eigenvalues, the columns of $\boldsymbol\Phi$
	become linearly independent. Therefore, if $\otime \geq \ncent$, the column rank of $\Obs_{\Tset}$ is $\ncent$, which results in an observable system.
	
	To extend this proof to matrices $\dualopApprox = \JorP\JorLa\JorP^{-1}$, note that:
	\begin{align*}
	\Obs_{\Tset} &= 
	\begin{bmatrix}
	\empK \dualopApprox^{\tindex_1}\\
	\cdots\\
	\empK \dualopApprox^{\tindex_\otime}
	\end{bmatrix}\\
	&=
	\begin{bmatrix}
	\empK \JorP\JorLa^{\tindex_1}\JorP^{-1}\\
	\cdots\\
	\empK \JorP\JorLa^{\tindex_\otime}\JorP^{-1}.
	\end{bmatrix}\\
	&=
	\catempK
	\catJorP
	\catJorLa^t
	\catJorP^{-1},
	\end{align*} 
	where $\catJorP\in\R^{\ncent\otime\times\ncent\otime}$, $\catJorLa^t\in\R^{\ncent\otime\times\ncent\otime}$, and
	$\boldsymbol{\JorP^{-1}}\in\R^{\ncent\otime\times\ncent\otime}$ are the block diagonal matrices associated with the system. 
	Since $\catJorP$ is an invertible matrix, the conclusions about the column rank drawn before still hold, and the system is observable. 
\end{proof}


% \begin{remark}
%  For the system corresponding to \ref{prop:1}, 
% \end{remark}
When the eigenvalues of the system matrix are repeated, it is not enough for $\empK$ to be shaded. 
% Intuitively, repeated eigenvalues correspond to coupled evolution\footnote{We use the term coupled evolution to denote dynamic evolution in which some eigen-modes evolve together \gXX{check rewrite}}, which requires an increased number of sensors to discern \gXX{what's the best way to explain this better? Without invariant subspaces it may not be clear to some why this matters, should we merge this with the discussion after the below prop?}. 
In the next proposition, we take a geometric approach and utilize the rational canonical form  of $\dualopApprox$ to obtain a lower bound on the number of sampling locations required. Let $\nevals$ be the number of unique eigenvalues of $\dualopApprox$, and let $\geomMult{\eval_i}$ denote the geometric multiplicity of eigenvalue $\eval_i$. Then the \emph{cyclic index} of $\dualopApprox$ is defined as $\minmeas = \max_{1\leq i\leq\nevals}{\geomMult{\eval_i}}$\cite{wonham1974linear}.

\begin{proposition}\label{prop:2}
 Suppose that the conditions in Proposition \ref{prop:1} hold, with the relaxation that
 the Jordan blocks $\left[\begin{smallmatrix}
                           \JorLa_1 &\cdots & \JorLa_{\JorMul}
                          \end{smallmatrix}\right]$ may have 
 repeated eigenvalues (i.e. $\exists \JorLa_i$ and $\JorLa_j$ s.t. $\eval_i = \eval_j$). Then there exist kernels $\kernel(x,y)$ such that the lower bound $\minmeas$ on the number of sampling locations $\nsamp$ is given by the cyclic index of $\dualopApprox$. In other words, the system in \eqref{k_measure} is observable if $ \nsamp \geq \ell$.
\end{proposition}
\begin{proof}
	\textbf{By Contrapositive.} We will show that if the number of sampling locations are $ \nsamp=\ell-1 $ (i.e. $ \nsamp < \ell$), then the system is not observable. Pick the Gaussian kernel in the dictionary of atoms framework,
	with sampling locations $x_i\in\sampSet$ and centers $c_j\in\shCent$, with the additional 
	property that $x_i\neq x_j \forall i,j\in\{1,\dots,\nsamp\}, i\neq j$.
	In this case, 
	$\empKShadFull$ has $\minmeas-1$ nonzero, linearly independent rows, and can be written as:
	\begin{align*}
	\empKShadFull &= \begin{bmatrix}
	k_{11} & k_{12} & \cdots & k_{1\ncent} \\
	\vdots & \vdots & \cdots & \vdots \\
	k_{(\minmeas-1)1} & k_{(\minmeas-1)2} & \cdots & k_{(\minmeas-1)\ncent} 
	\end{bmatrix}.
	\end{align*}
	Since the cyclic index is $\minmeas$, this implies that at least one eigenvalue, say $\eval$, has $\minmeas$ Jordan blocks. 
	%  For concreteness, suppose $\minmeas=2$, and let the size of the blocks be $i\times i$ and $j\times j$. 
	Define indices $j_1, j_2, \dots, j_{\minmeas} \in \{1,2,\dots,\ncent\}$ as the columns corresponding to the leading entries of the $\minmeas$ Jordan blocks corresponding to $\eval$. WLOG, let $j_1 = 1$. Using ideas similar to the last proof, we can write the observability matrix as:
	\begin{align*}
	\Obs_{\Tset}
	&:= 
	\begin{bmatrix}
	\kernel_{11}\eval^{\tindex_1}  & \cdots & \kernel_{1j_{\minmeas}}\eval^{\tindex_1} & \cdots\\
	\vdots & \ddots &\vdots & \ddots\\
	\kernel_{11}\eval^{\tindex_{\otime}}  & \kernel_{1j_{\minmeas}}\eval^{\tindex_{\otime}} & \cdots\\
	\vdots & \ddots & \vdots & \ddots\\
	\kernel_{(\minmeas-1)1}\eval^{\tindex_1}  \cdots & \kernel_{(\minmeas-1)j_{\minmeas}}\eval^{\tindex_1} & \cdots\\
	\vdots & \ddots &\vdots & \ddots\\
	\kernel_{(\minmeas-1)1}\eval^{\tindex_{\otime}}  & \cdots & \kernel_{(\minmeas-1)j_{\minmeas}}\eval^{\tindex_{\otime}} & \cdots
	\end{bmatrix}.
	\end{align*}
	Define $\evalvec:= \begin{bmatrix}\eval^{\tindex_1} & \eval^{\tindex_2} & \cdots \eval^{\tindex_{\otime}}\end{bmatrix}^T$. 
	Then the above matrix becomes:
	\begin{align*}
	\Obs_{\Tset}
	&:= 
	\begin{bmatrix}
	\kernel_{11}\evalvec  & \cdots & \kernel_{1j_2}\evalvec & \cdots & \kernel_{1j_{\minmeas}}\evalvec & \cdots\\
	\vdots & \ddots & \vdots & \ddots &\vdots & \ddots\\
	\kernel_{(\minmeas-1)1}\evalvec  & \cdots & \kernel_{(\minmeas-1)j_2}\evalvec & \cdots & \kernel_{(\minmeas-1)j_{\minmeas}}\evalvec & \cdots
	\end{bmatrix}.
	\end{align*}
	We need to show that one of the columns above can be written in terms of the others. This is equivalent to solving the linear system.
	\begin{align*}
	\begin{bmatrix}
	\kernel_{1j_1}\\
	\kernel_{2j_1}\\
	\vdots\\
	\kernel_{(\minmeas-1)j_1}
	\end{bmatrix}
	%   &=
	%   c_1
	%   \begin{bmatrix}
	%    \kernel_{1j_2}\\
	%    \kernel_{2j_2}\\
	%    \vdots\\
	%    \kernel_{(\minmeas-1)j_2}
	%   \end{bmatrix} 
	%   + \cdots + c_{\minmeas-1}
	%   \begin{bmatrix}
	%    \kernel_{1j_{\minmeas}}\\
	%    \kernel_{2j_{\minmeas}}\\
	%    \vdots\\
	%    \kernel_{(\minmeas-1)j_{\minmeas}}
	%   \end{bmatrix}\\
	&=
	\begin{bmatrix}
	\kernel_{1j_2} & \cdots & \kernel_{1j_{\minmeas}}\\
	\kernel_{2j_2} & \cdots & \kernel_{2j_{\minmeas}}\\
	\vdots & \ddots & \vdots\\
	\kernel_{(\minmeas-1)j_2} & \cdots & \kernel_{(\minmeas-1)j_{\minmeas}}\\
	\end{bmatrix} 
	\begin{bmatrix}
	c_1\\
	c_2\\
	\vdots\\   
	c_{(\minmeas-1)}
	\end{bmatrix}. 
	\end{align*}
	Since the kernel matrix on the RHS is generated from the Gaussian kernel, from \cite{micchelli1984interpolation}, 
	it's known that every principal minor of a Gaussian kernel matrix is invertible, which implies that $\Obs_{\Tset}$ cannot be observable. 
\end{proof}


We will give a concrete example to build intuition regarding this lower bound below. For now, we note the following:
\begin{figure}[t!]
	\begin{algorithm}[H]
		\caption{Measurement Map $\measmap$}
		\label{alg:measmap}
		\begin{algorithmic}
				\STATE {\bfseries Input:} $\dualopApprox\in\R^{\ncent\times\ncent}$
				\STATE Compute Rational Canonical Form, s.t. $\FrobC = \FrobP^{-1}\dualopApprox^T\FrobP$. Set $\FrobC_0:=\FrobC$, and
				$\ncent_0:=\ncent$. 
				\FOR{$i=1$ {\bfseries to} $\minmeas$}
				\STATE Obtain MP $\minpoly_i(\eval)$ of $\FrobC_{i-1}$. 
				This returns associated indices $\mmapInd{i}\subset\{1,2,\dots,\ncent_{i-1}\}$. 
				\STATE Construct vector $\linvec_i \in \R^{\ncent}$ such that 
				$\minpolyv_{\linvec_i}(\eval)=\minpoly_{i}(\eval)$ .
				\STATE Use indices $\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}$ to select matrix $\FrobC_i$. Set 
				$\ncent_i:= |\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}|$
				\ENDFOR
				\STATE Compute $ \premeasmap = [\linvec_1^T, \linvec_2^T,...,\linvec_{\minmeas}^T]^T$
				\STATE {\bfseries Output:} $\measmap =\premeasmap\FrobP^{-1}$
		\end{algorithmic}
	\end{algorithm}
\end{figure}


\begin{proposition}\label{prop:3}
Given the conditions stated in Proposition \ref{prop:2}, it is possible to construct a measurement map $\measmap \in \R^{\minmeas\times\ncent}$ for the system given by \eqref{k_measure}, such that the pair $(\measmap, \dualopApprox)$ is observable.
\end{proposition}
\begin{proof}
	The construction of the measurement map $\measmap$ is based on the rational canonical structure of $\dualopApprox^T$, which decomposes $ \linspace $ into $\dualopApprox^T$-cyclic direct summands such that $\linspace = \linspace_1 \oplus \cdots \oplus \linspace_\minmeas$, where $\minmeas$ is the cyclic index of $\dualopApprox$. Let $\minpolyv_{\linvec}$ be the minimal polynomial (m.p.) of $ \linvec $ (relative to $\dualopApprox^T$): it is then the unique monic polynomial of least degree such that $\minpolyv_{\linvec}(\dualopApprox^T)\linvec=0$. Let $\minpoly_1(\eval)$ be the m.p. of ${\dualopApprox^T}_{|\linspace_1}$: then $\degs(\minpoly_1(\eval)) < \ncent$. By the rational canonical structure theorem \cite{wonham1974linear}, there exists a vector $\widehat{\linvec}_1$, such that $\minpolyv_{\linvec_1}(\eval)=\minpoly_1(\eval)$. Similarly there exists a vector $\widehat{\linvec}_2$, such that $\minpolyv_{\linvec_2}(\eval)=\minpoly_2(\eval)$, where $\minpoly_2(\eval)$, is the minimal polynomial of ${\dualopApprox^T}_{|\linspace_2}$ and so on. Thus we can obtain $\minmeas$ such vectors that form the measurement map $\measmap = [\widehat{\linvec}_1, \widehat{\linvec}_2,\cdots, \widehat{\linvec}_\minmeas]^T$. Construction of these vectors $\widehat{\linvec}_i$, can be simplified by first performing the Jordan decomposition as $ \dualopApprox^T = \JorP\JorLa\JorP^{-1} $. Then the vectors $ \widetilde{\linvec_i},\ i\in \{1,\dots,\minmeas\}$ for $ \JorLa $, can be constructed such that the entries corresponding to the leading entries of Jordan blocks of $ \JorLa_{|\linspace_i} $ are nonzero. Such a construction ensures that the m.p. of vector $ \widetilde{\linvec_i}$ w.r.t $\JorLa_{|\linspace_i}$, is also the corresponding m.p. of 
	$\JorLa_{|\linspace_i}$. Finally, the required map can be obtained as $ \measmap = [\widetilde{\linvec_1}, \widetilde{\linvec_2},\dots,\widetilde{\linvec_\minmeas}]^T\JorP^{-1}$.
\end{proof}

The construction provided in the proof of Proposition \ref{prop:3} is utilized in Algorithm 1, which uses the rational canonical structure of $\dualopApprox$ to generate a series of vectors $\linvec_i\in\R^{\ncent}$, whose iterations $\{\linvec_1,\dots,\dualopApprox^{\acycdeg_1-1}\linvec_1,\dots,\linvec_{\minmeas},\dots,\dualopApprox^{\acycdeg_{\minmeas}-1}\linvec_{\minmeas}\}$ generate a basis for $\R^{\ncent}$.
Unfortunately, the measurement map $\measmap$, being an abstract construction unrelated to the kernel, does not directly select $\sampSet$. We will show how to use the measurement map to guide a search for $\sampSet$ in Remark \ref{rem:1} (in Appendix). For now, we state a sufficient condition for observability of a general system. 

\begin{theorem}\label{thm:1}
 Suppose that the conditions in Proposition \ref{prop:1} hold, with the relaxation that the Jordan blocks $\begin{bmatrix}\JorLa_1 & &\cdots & \JorLa_{\JorMul}\end{bmatrix}$ may have repeated eigenvalues. Let $\minmeas$ be the cyclic index of $\dualopApprox$. Define:
 \begin{align}\eqlabel{empKShadFull}
  \empKShadFull = \left[\begin{smallmatrix}
                    \empK^{{(1)}^T} & 
                    \cdots &
                    \empK^{{(\minmeas)}^T}
                  \end{smallmatrix}\right]^T
 \end{align}
 as the \emph{$\minmeas$-shaded matrix} (see Figure \ref{fig:shadedb}) which consists of $\minmeas$ shaded matrices with the property that any subset of
 $\minmeas$
 columns in the matrix are linearly independent from each
 other. Then system \eqref{k_measure} is observable if $\Tset$ has distinct values, and $|\Tset| \geq \ncent$.
\end{theorem}
\begin{proof}
	A cyclic index of $\minmeas$ for this system implies that there exists an eigenvalue $\eval$ that's repeated $\minmeas$ times. We prove the theorem for repeated eigenvalues of dimension 1: the same statement can be proven for repeated eigenvalues for Jordan blocks using the ideas in the proof of Proposition \ref{prop:1}. 
	WLOG, let $\empKShadFull$ have $\minmeas$ fully shaded, linearly independent rows, and, assume that the column indices corresponding to this eigenvalue are $\{1,2,\dots,\minmeas\}$. Define: 
	$\evalvec_i:= \begin{bmatrix}\eval_i^{\tindex_1} & \eval_i^{\tindex_2} & \cdots \eval_i^{\tindex_{\otime}}\end{bmatrix}^T$. Then,
	\begin{align*}
	\Obs_{\Tset}
	&:= 
	\begin{bmatrix}
	k_{11} \evalvec_1 & k_{12} \evalvec_2 & \cdots & k_{1\ncent} \evalvec_{\ncent}\\
	\vdots & \vdots & \ddots & \vdots\\
	k_{\minmeas 1} \evalvec_1 & k_{\minmeas 2} \evalvec_2 & \cdots & k_{\minmeas \ncent} \evalvec_{\ncent}
	\end{bmatrix}.
	\end{align*}
	Let $\evalvec_1 = \evalvec_2 = \cdots \evalvec_{\minmeas} := \evalvec$. 
	Focusing on these first $\minmeas$ columns of this matrix, this implies that
	we need to find constants $c_1,c_2,\dots, c_{\minmeas-1}$ such that:
	\begin{align*}
	\begin{bmatrix}
	k_{11}\\
	\vdots\\
	k_{\minmeas 1}
	\end{bmatrix}
	&= 
	c_1 
	\begin{bmatrix}
	k_{12}\\
	\vdots\\
	k_{\minmeas 2}
	\end{bmatrix}
	+ \cdots + 
	c_{\minmeas-1} 
	\begin{bmatrix}
	k_{1\minmeas}\\   
	\vdots\\
	k_{\minmeas \minmeas} 
	\end{bmatrix}.
	\end{align*}
	However, these columns are linearly independent by assumption, and thus no such constants exist, implying that $\Obs_{\Tset}$ is observable. 
\end{proof}

While Theorem \ref{thm:1} is a quite general result, the condition that any $\minmeas$ columns of $\empKShadFull$ be linearly independent is a very stringent condition. One scenario where this condition can be met with minimal measurements is in the case when the feature map $\fmapApprox(x)$ is generated by a dictionary of atoms with the Gaussian RBF kernel evaluated at sampling locations $\sampSetLong$ according to \eqref{fmap_dict}, where $x_i\in\dom\subset\R^d$, and $x_i$ are sampled from a non-degenerate probability distribution on $\dom$ such as the uniform distribution. For a semi-deterministic approach, when the dynamics matrix $\dualopApprox$ is block-diagonal, we can utilize a simple heuristic:

\begin{remark}\label{rem:1}
 Let $\dom$ be compact, $\shCent = \shCentLong$, $c_i\in\dom$, and let the approximate feature map be defined by \eqref{fmap_dict}. Consider the system \eqref{k_measure} with $\dualopApprox=\JorLa$, and let $\Tset = \{0,1,\dots,\ncent-1\}$. Then the measurement map $\measmap$'s values lie in $\{0, 1\}$; in particular, each row $\measmap^{(j)}$, $j\in\{1,\dots,\minmeas\}$, corresponds to a subspace $\fsubspaceC{j}$, generated by a subset of centers $\shCent^{(j)}\subset\shCent$. Generate samples $x_i^{(j)}$ to create a kernel matrix $\empK^{(j)}$ that is shaded only with respect to centers $\shCent^{(j)}$. Once this is done, move on to the next subspace $\fsubspaceC{j+1}$. When all $\minmeas$ rows of $\measmap$ are accounted for, construct the matrix $\empKShadFull$ as in \eqref{empKShadFull}. Then the resulting system $(\empKShadFull, \dualopApprox)$ is observable. 
\end{remark}


This heuristic is formalized in Algorithm 2. Note that in practice, the matrix $\dualopApprox$ needs to be inferred from measurements of the process $f_{\tindex}$. If no assumptions are placed on $\dualopApprox$, it's clear that at least $\ncent$ sensors are required for the system identification phase. Future work will study the precise conditions under which system identification is possible with less than $\ncent$ sensors.

% \begin{figure}[t!]
% 	\begin{algorithm}[H]
% 		\caption{Measurement Map $\measmap$}
% 		\label{alg:measmap}
% 		\begin{algorithmic}
% 			\begin{footnotesize} 
% 				\STATE {\bfseries Input:} $\dualopApprox\in\R^{\ncent\times\ncent}$
% 				\STATE Compute Frobenius canonical form, such that $ \FrobC  = \FrobP^{-1} \dualopApprox^T\FrobP$. Set $\FrobC_0:=\FrobC$, and
% 				$\ncent_0:=\ncent$. 
% 				%           Obtain subspaces 
% 				%           $\linspace_i\subset\R^{\ncent}$ s.t. $\R^{\ncent} = \linspace_1\oplus\cdots\oplus\linspace_{\minmeas}$.
% 				\FOR{$i=1$ {\bfseries to} $\minmeas$}
% 				\STATE Obtain MP $\minpoly_i(\eval)$ of $\FrobC_{i-1}$. 
% 				This returns associated indices $\mmapInd{i}\subset\{1,2,\dots,\ncent_{i-1}\}$. 
% 				%             \STATE Use $\mmapInd{i}$ to select submatrix $\widetilde{\FrobP}_i$, where an index $j\in\mmapInd{i}$ 
% 				%                    indicates selection of the row and column associated to $j$.
% 				\STATE Construct vector $\linvec_i \in \R^{\ncent}$ such that 
% 				$\minpolyv_{\linvec_i}(\eval)=\minpoly_{i}(\eval)$ .
% 				\STATE Use indices $\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}$ to select matrix $\FrobC_i$. Set 
% 				$\ncent_i:= |\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}|$
% 				\ENDFOR
% 				\STATE Compute $ \premeasmap = [\linvec_1^T, \linvec_2^T,...,\linvec_{\minmeas}^T]^T$
% 				\STATE {\bfseries Output:} $ \measmap =\premeasmap\FrobP^{-1}  $
% 			\end{footnotesize}
% 		\end{algorithmic}
% 	\end{algorithm} 
% \end{figure}

\begin{figure}[t!]
	\begin{algorithm}[H]
		\caption{Sampling locations set $ \sampSet $}
		\label{alg:samples}
		\begin{algorithmic}
				\STATE {\bfseries Input:} $ \dualopApprox =\FrobC $, lower bound $\minmeas$
				%		\STATE Check $k=\max_i \nu(\lambda_i)$
				\STATE Decompose $ \FrobC $ to generate invariant subspaces $ \fsubspaceC{j} $, $ j\in \{1,2,\dots,\minmeas\} $ (see section \ref{sec:invariant})
				\FOR{$j=1$ {\bfseries to} $\minmeas$}
				\STATE Obtain centers $\shCent^{(j)}$ w.r.t subspace $ \fsubspaceC{j} $,
				\STATE Generate samples $x_i^{(j)}$ to create a kernel matrix $\empK^{(j)}$ that is shaded only with respect to centers $\shCent^{(j)}$
				\ENDFOR
				\STATE {\bfseries Output:} Sampling locations set  $\sampSet =\{x^{(1)}, x^{(2)}\cdots,x^{(l)}\} $.
		\end{algorithmic}
	\end{algorithm}
\end{figure}

\begin{figure}[t!]
	\centering
	\begin{algorithm}[H]
		\caption{Kernel Observer (Transition Learning)}
		\label{alg:egp_trans}
		\begin{algorithmic}
				\STATE {\bfseries Input:} Kernel $\kernel$, basis centers $\shCent$, final time 
				step $\ftime$. 
				\WHILE{$\tindex \leq \ftime$}
				\STATE $1)$ Sample data $\{y^i_{\tindex}\}_{i=1}^{\ncent}$ from $f_{\tindex}$. 
				\STATE $2)$ Solve for $\estweight_\tindex$ by using least squares on $\meas_{\tindex} = \obsMat \estweight_{\tindex}$. 
				\STATE $3)$ Store weights $\estweight_\tindex$ in matrix $\W\in\R^{\ncent\times \ftime}$.
				\ENDWHILE
				\STATE To infer $\dualopApprox$, define matrix $\Phi = \W^T\W$. Then:
				\FOR{$i=1$ {\bfseries to} $\ncent$}
				\STATE At step $i$, solve system
				\begin{align}
				\dualopApprox^{(i)} = \left(\left(\Phi + \la I\right)^{-1}(\W^T\W^{(i)})\right)^T,
				\end{align}
				where $\dualopApprox^{(i)}$, and $\W^{(i)}$ are the $i$th columns of $\dualopApprox$ and $\W^{(i)}$ respectively. 
				\ENDFOR
				\STATE Compute the covariance matrix  $\estcontrolop$ of the observed 
				weights $\W$. 
				\STATE {\bfseries Output:} estimated transition matrix $\dualopApprox$, predictive covariance    
				matrix $\estcontrolop$. 
		\end{algorithmic}
	\end{algorithm}
	\vspace{-0.3in}
\end{figure}

\begin{figure}[t!]
	\begin{algorithm}[H]
		\caption{Kernel Observer (Monitoring and Prediction)}
		\label{alg:egp_inf}
		\begin{algorithmic}
				\STATE {\bfseries Input:} Kernel $\kernel$, basis centers $\shCent$, 
				estimated system matrix $\dualopApprox$, estimated covariance matrix $\estcontrolop$.
				\STATE {\bfseries Compute Observation Matrix:} Compute the cyclic index $\minmeas$ of $\dualopApprox$, and compute $\empK$.
				\STATE {\bfseries Initialize Observer:} Use $\dualopApprox$, $\estcontrolop$, and $\empK$ to initialize a state-observer (e.g. Kalman filter (KF)) on $\fspaceApprox$.
				\WHILE{ measurements available }   
				\STATE 1) Sample data $\{y^i_{\tindex}\}_{i=1}^{\nsamp}$ from $f_{\tindex}$.
				\STATE 2) Propagate KF estimate $\estweight_{\tindex}$ 
				forward to time $\tindex+1$, correct using measurement feedback with $\{y^i_{\tindex+1}\}_{i=1}^{\nsamp}$. 
				\STATE 3) Output predicted function $\widehat{f}_{\tindex+1}$ of KF.
				\ENDWHILE   
		\end{algorithmic}
	\end{algorithm}
	\vspace{-0.2in}
\end{figure}
% 
% \begin{figure}[tbh]
% 	\begin{algorithm}[H]
% 		\caption{Kernel Controller}
% 		\label{alg:egp_control}
% 		\begin{algorithmic}
% 				\STATE {\bfseries Input:} Kernel $\kernel$, basis points $\shCent$,
% 				estimated system matrix $\estsysop$, estimated covariance matrix $\estcontrolop$, and function $f_{\text{ref}}$ 
% 				to drive initial function to.    
% 				\STATE {\bfseries Initialize Observer:} (see Algorithm \ref{alg:egp_inf}). 
% 				\STATE {\bfseries Initialize Controller:} Use Jordan decomposition of $\estsysop$ to obtain  no. of control locations $\controlCent$, compute kernel matrix $\empKCD\in\R^{\ncontrol\times\ncent}$ between $\controlCent$ and $\shCent$, and initialize controller (e.g. LQR) utilizing $(\estsysop, \estcontrolop)$.
% 				\WHILE{ measurements available }   
% 				\STATE 1) Sample data $\{y^i_k\}_{i=1}^{\nsamp}$ from $f(x,\tindex)$.    
% 				\STATE 2) Utilize observer to estimate $\estweight_{\tindex+1}$.
% 				\STATE 3) Use $\estweight_{\tindex+1}$ and $f_{\text{ref}}$ as input to controller to get feedback. 
% 				\ENDWHILE   
% 		\end{algorithmic}
% 	\end{algorithm}
% 	\vspace{-0.2in}
% \end{figure}

\begin{figure*}[ht!]
\centering
\resizebox{1\textwidth}{!}{
\framebox[1.1\textwidth]{
 \begin{tikzpicture}[->,>=stealth',auto,node distance=1cm,
  thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
  \node[inner sep=0pt,label=below:{\scriptsize Physical 
                                  sampling locations},draw] (physical) at (-8,-0.3)
    {\includegraphics[width=0.19\textwidth,
                      height=0.13\textwidth]{figures/lab.png}};
  \node[inner sep=0pt,label=below:{\scriptsize Data locations},draw] (data) at (-4.25,-0.3)
    {\includegraphics[width=0.19\textwidth,
                      height=0.13\textwidth]{figures/locations.pdf}};
  \node[inner sep=0pt,label=below:{\scriptsize Functional inference 
                                   (for $\estsysop$)},draw] (inference) at (-0.5,-0.3)
    {\includegraphics[trim=2.4cm 2.6cm 3.4cm 1.5cm,clip,
                      width=0.19\textwidth,
                      height=0.13\textwidth]{figures/kernel_evol_cauchy.pdf}};    
  \node[inner sep=0pt,label={[align=center]
       below:{\scriptsize Sensor location selection after\\[-1.5\jot]
              \scriptsize basis decomposition ($\minmeas=3$)}},draw] 
       (sensors) at (3.25,-0.3)
       {\includegraphics[width=0.19\textwidth,
                         height=0.13\textwidth]{figures/locations_sensors.pdf}};                          
  \node[inner sep=0pt,label=below:{\scriptsize Physical sensor placement},draw] (placement) at (6.75,-0.3)
    {\includegraphics[width=0.19\textwidth,
                      height=0.13\textwidth]{figures/lab_sensors2.png}};                                                
  \path[every node/.style={font=\sffamily\small}]
    (physical) edge node [right] {} (data)
    (data) edge node [right] {} (inference)
    (inference) edge node [right] {} (sensors)
    (sensors) edge node [right] {} (placement);
\end{tikzpicture}

} % end framebox
} % end resizebox
\caption{
Overall description of how the kernel observer fits in the sensing framework. Physical 
locations are mapped to data locations, over which historical data is collected 
as a time series. Functional inference is performed over $\fspaceApprox$ to 
solve for $\estsysop$. The measurement operator $\empK$ is then computed 
(see Figure \ref{fig:sensplace}), leading to sensor placement. 
}\label{fig:overall_system}

\end{figure*}


\begin{figure*}[ht!]
  \begin{minipage}{\textwidth}
  \centering
  \begin{minipage}{0.47\textwidth}
  \resizebox{1\textwidth}{!}{
  \framebox[1.2\textwidth]{  
  \begin{tikzpicture}[scale=1.0, every node/.style={minimum size=1cm},on grid]    
	\begin{scope}[scale=\diagscale, xshift=-250, yshift=0]
		% The frame:
		\fill[white,fill opacity=.85] (0,0) rectangle (7,7); % Opacity
		\draw[black, thin] (0,0) rectangle (7,7); 
		 % Agents:
		\draw [fill=red]
			(4.7,2.7) circle (.2) % Firms
			(2,2) circle (.2) % Households
			(3.5,4.6) circle (.2); % Banks
		\fill[black]
			(0.5,6.5) node[right, scale=\diagtexttop] { \textbf{Compute cyclic index:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\boldsymbol\minmeas = \mathbf{2}$}
			(4.5,2.1) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
			(2.5,1.4) node[left, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$}
			;
        % draw lines representing possible choices of decomposition                
        \draw[thick](1.3,4.1) to (6.5,4.1);
        \draw[thick](2.2,0.5) to (6.5,5.4);
        \draw[thick](2.5,0.5) to (2.5,5.3);
	\end{scope} 
	\begin{scope}[scale=\diagscale, xshift=0, yshift=0]
		% The frame:
		\draw[black, thin] (0,0) rectangle (7,7); 
		% Agents:
		\draw [fill=blue]
			(4.7,2.7) circle (.2) % Firms
			(2,2) circle (.2); % Households
		\draw [fill=green]			
			(3.5,4.6) circle (.2); % Banks
		 % Labels:
		\fill[black]
		        (0.5,6.5) node[right, scale=\diagtexttop] { \textbf{Compute meas. map:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\boldsymbol\measmap =[\mathbf\linvec_1, 
			                        \mathbf\linvec_2]^T\boldsymbol\JorP^{-1}$}
% 			(4.5,2.1) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
% 			(2.5,1.4) node[left, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
% 			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$}
			;
	\end{scope} 
	\begin{scope}[scale=\diagscale, xshift=250, yshift=0]
		% The frame:
		\draw[black, thin] (0,0) rectangle (7,7); 
		% Agents:
		\draw [fill=blue, opacity=0.4]
			(4.7,2.7) circle (.2) 
			(2,2) circle (.2)
			;
		\draw [fill=blue]
			(3.2,2) rectangle (3.55,2.35)
			; 
			\node (one) at (3.37,1.5) {\scriptsize$ 1 $};
		\draw [fill=green, opacity=0.4]
			(3.5,4.6) circle (.2)
			;
		\draw [fill=green]			
			(3.0,4.0) rectangle (3.35,4.35)
			;
			\node (two) at (3.17,3.5) {\scriptsize$ 2 $};
		\fill[black]
		        (0.5,6.5) node[right, scale=\diagtexttop] { \textbf{Sensor loc. selection:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\mathbf{x_1, \ x_2\in}\boldsymbol\dom \boldsymbol\Rightarrow \empKShadFull$}
% 			(4.5,2.1) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
% 			(4.5,2.1) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{x_2})$}
% 			(2.5,1.4) node[left, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
% 			(1.4,4.7) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{x_1})$}
% 			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$}
			;
	\end{scope}	
  \end{tikzpicture}
  } % end framebox
  } % end resizebox  
  \end{minipage}
  \begin{minipage}{0.47\textwidth}
  \resizebox{1\textwidth}{!}{
  \framebox[1.2\textwidth]{  
  \begin{tikzpicture}[scale=1.0, every node/.style={minimum size=1cm},on grid]  
	\begin{scope}[scale=\diagscale, xshift=-150, yshift=0]
		% The frame:
		\fill[white,fill opacity=.85] (0,0) rectangle (7,7); % Opacity
		\draw[black, thin] (0,0) rectangle (7,7); 
		 % Agents:
		\draw [fill=red]
			(4.7,2.7) circle (.2) % Firms
			(2,2) circle (.2) % Households
			(3.5,4.6) circle (.2); % Banks
		\fill[black]
			(0.5,6.5) node[right, scale=\diagtexttop]{\textbf{Compute cyclic index:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\boldsymbol\minmeas = \mathbf{2}$}
			(4.5,2.1) node[right,scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
			(2.5,1.4) node[left,scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$};
        % draw lines representing possible choices of decomposition                
        \draw[thick](1.3,4.1) to (6.5,4.1);
        \draw[thick](2.2,0.5) to (6.5,5.4);
        \draw[thick](2.5,0.5) to (2.5,5.3);
	\end{scope} 
	\begin{scope}[scale=\diagscale, xshift=150, yshift=0]
		% The frame:
		\draw[black, thin] (0,0) rectangle (7,7); 
		% Agents:
		\draw [fill=blue, opacity=0.4]
			(4.7,2.7) circle (.2) % Firms
			(2,2) circle (.2)
			;
		\draw [fill=blue]
			(1.17,1.1) rectangle (1.17+0.35,1.1+0.35)
			(5.0,1.8) rectangle (5.35,2.15)
                        ;
           \node (one) at (1.31,0.7) {\scriptsize$ 1 $};
           \node (two) at (5.17,1.3) {\scriptsize$ 2 $};
		\draw [fill=green, opacity=0.4]
			(3.5,4.6) circle (.2)
			;
		\draw [fill=green]
                        (3.0,4.0) rectangle (3.35,4.35)
			;
			\node (three) at (3.17,3.5) {\scriptsize$ 3 $};
		 % Labels:
		\fill[black]
		        (0.5,6.5) node[right, scale=\diagtexttop] { \textbf{Random sampling:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\mathbf{x_1, x_2, x_3\in} \boldsymbol\dom \Rightarrow \empKShadFull$}
% 			(4.5,2.1) node[right,scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
% 			(2.5,1.4) node[left,scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
% 			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$}
			;
	\end{scope} 
  \end{tikzpicture}
  } % end framebox
  } % end resizebox                      
  %\captionof{figure}{Sensor selection using random sampling} \label{fig:sselec_rand}
  \end{minipage} 
  \end{minipage} 
  \caption{
  Diagram demonstrating sensor placement using the measurement map or random sampling approaches. 
  The circles represent data locations associated to bases (e.g. $c_j\Leftrightarrow\fmap(c_j)$) 
  and the squares represent sensor locations (e.g. $x_i\Leftrightarrow\fmap(x_i)$) .
  The cyclic index ($\minmeas=2$) indicates how many possible couplings of bases exist, 
  which can be represented 
  as a choice of $\binom{\ncent}{\minmeas}$ hyperplanes in $\dom$. If the measurement map is 
  computed (left), the correct couplings are chosen (green vs. blue), and a smaller number of sensors (2) can be placed.
  Alternatively, random sampling (right) is more computationally efficient, but generally
  requires more sensors (3). 
  }\label{fig:sensplace}
\end{figure*}



\subsection{Discussion of Theoretical Results}\label{sec:discussion}
The systems-theoretic approach taken in this paper reveals something rather surprising: functions with complex dynamics (with a small cyclic index) can be recovered with less sensor placements than functions with simpler dynamics. Although seemingly counterintuitive, it becomes clear that this is because complex dynamics, which are characterized by a lower geometric multiplicity of the eigenvalues, ensure that the orbit $\orbit := \{\dualopApprox\weight_{\tindex}\}_{\tindex\in\Tset}$ traverses a greater portion of $\R^{\ncent} \equiv \fspaceApprox$ and thus that fewer sensors can recover more geometric information. On the other hand, in `simpler' functional evolution, $\orbit$ evolves along strict subspaces of $\R^{\ncent}$, and so more independent sensors are required to infer the same amount of information. 

In the case described in Remark \ref{rem:1}, we have a set of centers $\shCent=\shCentLong$, which generate the bases $\Atoms = \{\fmap(c_1), \cdots , \fmap(c_{\ncent})\}$. Let the cyclic index be $\minmeas$: this implies that there exist $\minmeas$ subsets $\atomSubset{i}$ of $\Atoms$ with at least one element $\fmap(c_j)$ each, leading to $\binom{\ncent}{\minmeas}$ possible choices: Figure \ref{fig:sensplace} represents these choices as hyperplanes separating the subsets. 
The measurement map described in Alg. 1 in \cite{Kingravi16_NIPS} induces this \emph{decomposition of bases} $\Atoms = \{\atomSubset{1},\dots,\atomSubset{\minmeas}\}$ in polynomial time. Further, each subset $\atomSubset{i}$ is directly associated to a subset of centers $\centerSubset{i}\subset\shCent$, which allows us to pick targeted sensor locations $x_i\in\dom$. In particular, for radially symmetric kernels such as the Gaussian, the centroid of the convex hull of $\centerSubset{i}$ is sufficient for generating a sensor placement. The measurement map is a significant theoretical insight into sensor placement for dynamically changing environments, because it directly takes into account the dynamics of the process. Of course, in practice, this may be too expensive for approximate feature spaces with $\ncent$ very large, so one can use random sampling to generate the sensor locations instead, at the cost of $\nsamp$ being larger than $\minmeas$. The advantage here though is that since random sampling is computationally inexpensive, different choices of sensor placements can be generated and evaluated relatively quickly.

Another point to note is that since the collection of bases $\{\fmapApprox_i(x)\}_{i=1}^{\ncent}$ determines the richness of the function space $\fspaceApprox\approx\fspace$ we operate in, it determines the fidelity of the model approximation to the true time-varying function. As a consequence, observability of the system in $\fspaceApprox$ refers to the best possible approximation in $\fspaceApprox$. The greater the number of bases, the higher the dimensionality, which results in greater model fidelity, but which may require a much greater number of measurements for state recovery. This is where the lower bounds presented in the paper are particularly useful, because they show that for functional evolutions corresponding to certain $\dualopApprox$, \emph{the number of sensor placements are essentially independent of the dimensionality $\ncent$}, but depend rather on the cyclic index of $\dualopApprox$.

Figure \ref{fig:overall_system} gives an overall picture on the process of generating a kernel observer, while Figure \ref{fig:sensplace} gives two approaches to sensor selection in our framework. The measurement map approach can generate a smaller set of sensors than the random placement approach, but comes at an additional computational cost. 

\subsection{Random Sensor Placement}\label{sec:random_results}
We now elaborate on how the challenging problem of sensor placement can be tackled through random selection. This process of random selection is a product of the kernel observer model described above. We present the theoretical background required to prove Theorem \ref{thm:r1}, which states the expected number of randomly placed sensors required to monitor a given spatiotemporal process, and Theorem \ref{thm:r2}, which determines the probability with which optimal sensor placement is ensured given that, $\nsamp$ number of sensors have been placed. 
%Otherwise to the best of our knowledge there does not exist any sensor placement design based on random selection.%Thus, the kernel observer is a modeling solution for spatiotemporal processes that also guides deterministic as well as random placement of sensors.

As discussed earlier, we work with an approximate feature space $ \fspaceApprox $, with the corresponding transition operator $ \dualopApprox: \fspaceApprox \rightarrow \fspaceApprox $, representing finite-dimensional functional evolution. To achieve observability for the pair ($ \dualopApprox , \empK $), row vectors of the corresponding observability matrix, $ \Obs $, should form the basis for the $ \R^\ncent $-dimensional space $ \fspaceApprox $. According to the rational canonical structure Theorem \cite{wonham1974linear}, $\dualopApprox$ can successively decompose the dual space $ \R^\ncent $ into subspaces, $\linspace_i \subset \linspace$, $i\in \{1,\dots,\minmeas\}$, with properties, i) $\linspace = \linspace_1 \oplus ... \oplus \linspace_{\minmeas}$, ii) $\dualopApprox\linspace_i \subset \linspace_i$, and iii) $\dualopApprox|\linspace_i, i \in \{1,\dots,\minmeas\}$, are cyclic. The integer $\minmeas$ is unique and is called the \emph{cyclic index of $\dualopApprox$}.  Each of these properties contribute towards the theorem on the number of random samples required to achieve observability. The first property shows that the space $ \R^\ncent $ can be decomposed into $ \ell $ independent subspaces. The second property shows that the vector $ \linvec_i \in \linspace_i $ stays in $ \linspace_i $ even when operated upon by $ \dualopApprox $. Thus, to generate bases for $ \R^\ncent $, one needs at least $ \ell $ vectors, say, $ \linvec_1, \dots,\linvec_\ell $, with respect to each subspace $ \linspace_1,\dots,\linspace_\ell $. This holds due to the third property, but requires that the vectors  $ \linvec_1, \dots,\linvec_\ell, $ are the cyclic generators of their corresponding subspaces. Our analysis is based on whether a randomly selected sensor can generate a cyclic generator. To examine this, recall that a row vector $ \empK_{(i)} $ generated by a randomly selected sensor location $ x_i $ takes the form,
\begin{equation}\label{eq:rowvec}
\empK_{(i)} = \bbm k(x_i,c_1),\dots,k(x_i,c_\ncent) \ebm.
\end{equation}
Here, for radial kernels for example, the entries corresponding to the centers closer to $ x_i $ tend to be non-zero, whereas the others tend to be zero. 
% 	Note this holds since in general kernel function depends upon the metric of distance between its argument, for example the squared exponential kernel. 
% 	This limits the capability of a cyclic generator being obtained from a random sensor, however the latter certainly generates a non-zero entry for a closest sensor data point. 
The rows $\empK_{(i)}$ from random sensor placement must be able to generate a basis for a subspace $ \linspace_i $, and thus must be cyclic generators.  We will derive the expected number of random sensor placements sufficient for observability for the case where $ \dualopApprox = \JorLa $ and then attempt to generalize the result for any $ \dualopApprox $. Note $ \JorLa $ is a block diagonal Jordan form.  In this case, the cyclic generator for each subspace $ 
\linspace_i $, is a vector $ \linvec_i $ with non-zero entries corresponding to the leading entry of the Jordan blocks of $ 
\linspace_i $. 
%An example of a subspace, and  its cyclic generator is, 
%\begin{align}
%\linspace_1 & = \begin{bmatrix}{
%1 & 1 & 0 & 0\\
%0 & 1 & 1 & 0\\
%0 & 0 & 1 & 0\\
%0 & 0 & 0 & 2}
%\end{bmatrix}, \quad  \linvec_1 = \begin{bmatrix}{
%0 \\
%0 \\
%s \\
%s'},
%\end{bmatrix},  \nonumber 
%\end{align}
%where $s,s'$ are non-zero. 

Overall, our construction is as follows: for each subspace $ \linspace_i $, let $ \shCent_{\linspace_i} \subset \shCent $ be the centers corresponding to those leading entry of Jordan blocks: then the minimum number of random samples required to generate the bases for $ \linspace_i $ is equal to the number of Jordan blocks comprising $ \linspace_i $. Altogether, the minimum number of random samples required to generate a basis for $ \R^\ncent $ is equal to the total number of Jordan blocks in $ \dualopApprox $. Let $ \rands $ be the total number of Jordan blocks in $ \dualopApprox  $, then
%	For entire space we obtain the measurement map $ \measmap = [\linvec_1^T, \linvec_2^T,...,\linvec_{\minmeas}^T]^T $.
\begin{equation}\label{rands}
\rands = \sum_{\lambda \in \sigma(\dualopApprox)} \gamma_{\dualopApprox}(\lambda) \qquad %\geomMult_\lambda
\end{equation}
where $\sigma(\dualopApprox) $ represents the spectrum of $ \dualopApprox $, whose elements are the eigenvalues of $ \dualopApprox $, and $ \gamma_{\dualopApprox}(\lambda) $ is the geometric multiplicity corresponding to the eigenvalue $ \lambda $, which is also equal to the total number of Jordan blocks corresponding to the eigenvalue $ \lambda $. Define a set of centers $ \shCent_\rands $ with elements $ \{c_1, c_2,\dots, c_\rands\} $, to be the centers corresponding to the leading entries of  the Jordan blocks.
For sensor location $x\in\dom$, and $ \epsilon > 0 $, let $\kernel(x, c_j) > \epsilon$, denote the region $\dom_j\subset\dom$, such that the kernel evaluation with respect to center $c_j$ is greater than $\e$, that is $ \dom_j \equiv \{x\in \dom:\kernel(x, c_j) > \epsilon\} $. We define
$ p_{\e} $ as:
\begin{equation}\label{ppp}
p_{\e} = \min_{c_j \in \shCent_\rands} \frac{\measure(\kernel(x, c_j) > \epsilon)}{\measure(\dom)},
\end{equation}
where $\measure$ is a measure in the real analysis sense. Hence, $p_{\e}$ corresponds to a lower bound on the probability that a random sample lies within the $ \epsilon-$shaded region of a particular center $ c_j$. With all of this in place, we can prove the following theorem.

\begin{theorem}\label{thm:r1}
	Given the spatiotemporal function $ f(x,\tindex) $ with $ x \in \dom \subseteq  \mathbb{R}^\dimI, \tindex\in \mathbb{Z}^+  $ its kernel observer model \eqref{k_measure}, and a tolerance parameter $\e>0$, the expected number of randomly placed sensor locations required to achieve observability for the pair $ (\empK,\dualopApprox) $ is $ \rands/{p_{\e}} $ where $ \rands $ is the summation over geometric multiplicities of each $ \lambda \in \sigma(\dualopApprox) $  given by  (\ref{rands}).
\end{theorem}
\begin{proof}
	For each random sample, the probability that it lies within the $ \epsilon- $shaded region of a particular center $ c_j \in \shCent_\rands$
	is at least $ p_{\e} $. The series of random samples can be considered as Bernoulli trials in which $p_{\e}$ is the probability of a successful outcome. Note this is assuming worst case scenario that the intersection between any two $ \epsilon $-shaded region of centers belonging to the set  $ \shCent_\rands $ is empty. Observability for the pair $ (\empK, \dualopApprox) $ is achieved after $ \rands  $ successful outcomes are obtained because each success ensures a row vector with non-zero entry corresponding to the leading entry of the Jordan block. 
	
	Let $ X_1, X_2, \dots, X_\nsamp $ be i.i.d. random variables whose common distribution is the Bernoulli distribution with parameter $p_{\e}$. The random variable $ X = X_1 + X_2+ \dots+ X_\nsamp $ denotes the number of success after $ \nsamp $ random samples. Since each $ X_i $ has the Bernoulli distribution, $ X $ will have binomial distribution, 
	\begin{equation*}
	P(X=h) = {\nsamp \choose h}p_{\e}^h (1-p_{\e})^{\nsamp-h},
	\end{equation*}
	in which $ h  $ is the number of success. The expectation of the binomial distribution, that is the expected number of success is $ Np_{\e} $, and thus the expected number of trials required will be $ N = \rands/p_{\e}$.
\end{proof}

\begin{theorem}\label{thm:r2}
	Given the spatiotemporal function $ f(x,\tindex) $ with $ x \in \dom \subseteq  \mathbb{R}^\dimI, \tindex\in \mathbb{Z}^+  $, its kernel observer model \eqref{k_measure}, a tolerance parameter $\e>0$, summation over geometric multiplicities of each $ \lambda \in \sigma(\dualopApprox) $ denoted by $ \rands  $ as in  (\ref{rands}), and a constant $ \delta \in (0,1] $, the probability that pair $ (\empK, \dualopApprox) $ is unobservable after the selection of $ \nsamp $ random sensors is at most $ e^{\frac{-1}{2}(\nsamp p_{\e}-2\rands)} $, where $ p_{\e} $ is given by (\ref{ppp}) and \rr{$ \nsamp > 2\rands/p_{\e} $}.
\end{theorem}
\begin{proof}
	The random variable $ X $ from the proof of Theorem 1 has a binomial distribution, which enables the application of a Chernoff-type bound on its tail probabilities. A well known result  on multiplicative Chernoff bound \cite{motwani2010randomized} is directly applied to establish this Theorem. If $ X $ is binomially distributed, $ \delta \in (0,1] $, and $ \meanDist = \mathbb{E}[X] $, then $ P[X\leq(1-\delta)\meanDist] \leq \exp(-\meanDist \delta^2/2) $, in which we let $ \delta = 1-\frac{\rands}{\nsamp p_{\e}}$.  The expression in the exponent can be simplified to $ -\frac{1}{2}\nsamp p_{\e}+\rands - \frac{\rands^2}{2\nsamp p_{\e}} $, using $ \meanDist = \nsamp p_{\e} $. Note that $ e^{\frac{-\rands^2}{2\nsamp p_{\e}}} \leq 1$. This implies that,
	\begin{equation*}
	\exp(-\meanDist \delta^2/2) =  e^{-\frac{1}{2}(\nsamp p_{\e}-2\rands)}.e^{\frac{-\rands^2}{2\nsamp p_{\e}}}\leq e^{-\frac{1}{2}(\nsamp p_{\e}-2\rands)}
	\end{equation*}
	Note, $ (1-\delta)\mu = \rands $, hence we obtain that $ P[X\leq\rands] \leq e^{-\frac{1}{2}(\nsamp p_{\e}-2\rands)}. $
\end{proof}


For the case when $ \dualopApprox \neq \JorLa $, a change of basis can be used to obtain $ \JorLa = P^{-1}\dualopApprox P $, where $ P $ is the projection map. There are two challenges in performing the above analysis for $\JorLa$ so obtained: first, the leading entries of Jordan blocks do not directly correspond to the centers $\{ c_1,\dots,c_\ncent\}$ which was the case for $ \dualopApprox = \JorLa $. Second, although we can obtain the transformation of the row vector (\ref{eq:rowvec}) using the projection map $P$, we can no longer arrive at the definition of the probability $p_{\e}$ as in  (\ref{ppp}). The existence of the similarity transform hints that the results in Theorems \ref{thm:r1}-\ref{thm:r2} should hold for any $ \dualopApprox$, but the mathematical tools utilized in the paper seem to be insufficient to prove them. However, we present some empirical evidence for these claims for when $ \dualopApprox \neq \JorLa $  in the empirical results section.%Section \cite{Maske18_ACC}.

\subsection{\rr{Invariant Subspaces}}\label{sec:invariant}

\rr{An invariant subspace is a subspace $ \fsubspaceC{j} $ which is preserved by the transition operator $\sysop$, that is $\sysop(\fsubspaceC{j}) \subseteq \fsubspaceC{j}$. The whole space is trivially an invariant subspace, but for our algorithms we are interested in finding the smallest non-zero subspaces.} One way of understanding the invariant subspaces concept is to say that information contained in an invariant subspace never leaves that invariant subspace. We hypothesize that the kernel centers associated with the invariant subspaces of a spatiotemporally evolving system are generally associated with spatial regions in the domain (and not just homogeneously spread throughout, or mixed with other invariant subspaces). This hypothesis makes sense both physically and mathematically. In physics, the \emph{principle of locality} states that an object is only directly influenced by its immediate surroundings \cite{berkovitz2007action}. If this is the case (and there is no reason to believe that it is not, apart from certain quantum dynamics situations), and the E-GP model accurately captures the physics of the system, then information (measurable phenomena) may only travel continuously from one point in the domain to another. \rr{Mathematically the hypothesis makes sense, since a value at any one point in the domain is influenced by the weights of multiple nearby centers, we would expect centers to be connected dynamically to those nearest. A ``low'' area between two nearby centers, where measurements are consistently close enough to zero to be indistinguishable from noise, might mark the boundary between two distinct invariant subspaces.}

The Jordan form of a $n\times n$ matrix $\dualopApprox$ is block diagonal, and therefore gives a decomposition of the $n$ dimensional Euclidean space into invariant subspaces of $\dualopApprox$. The cyclic index, which can be found by counting the geometric multiplicities of eigenvalues in $\dualopApprox$, gives the number of invariant subspaces. \rr{However, in reality, the square matrices generated from the data using our regression methods do not usually decompose neatly into a few precise Jordan blocks, but instead usually end up as one large, useless block. This drives the need for an algorithm that can divide the system into invariant subspaces even when the boundaries between such spaces are not mathematically exact.}

%Each block in the Jordan normal form has a set of corresponding eigenvectors and an eigenvalue with geometric multiplicity. When we transform the former back into the domain space, we obtain complex-valued functions which are the Koopman modes of the system. These provide an image of what kind of structures we see in the dynamics. The eigenvalues describe the frequency with which these structures oscillate between their real and imaginary forms, as well as their exponential growth or decay in magnitude. %In this section, we demonstrate our method on systems with eigenvalues primarily on the unit circle (neither growing nor decaying).


\subsubsection{$k$-Invariant Subspaces Clustering}
In response to the need for an algorithm that can separate invariant subspaces in the linear model of a system generated by data, we have developed what we call ``$k$-invariant subspaces clustering''. The intuition behind our algorithm is to replace the Euclidean distance in the well-known $k$-means algorithm with a different metric of ``nearness'', namely one corresponding with the dynamic connections in the space. The $\dualopApprox$ matrix provides easy access to these: its rows $\dualopApprox_{i\ast}$ indicate which centers inform the i\textsuperscript{th} value of $\weight_{tindex+1}$, and its columns $\dualopApprox_{\ast j}$ indicate what centers will be informed by the j\textsuperscript{th} value of $\weight_{\tindex}$. However, these values do tend to be biased towards the eigenmodes with higher frequencies (i.e. those whose eigenvalues have a greater polar angle) and those which grow exponentially. To  control for this, we chose to modify the eigenvalues of the matrix in the following way: (1) We zeroed any eigenvalue that is clearly inside (not on) the unit circle, (2) We unitized the remaining eigenvalues, and (3) We adjusted the frequency of the remaining eigenvalues on the unit circle to either $\pm \frac{\pi}{4}$. If the eigen-decomposition of the original was $\dualopApprox = UDU^{-1}$, then we can reconstruct a new matrix with our modified eigenvalues $\bar D$ as $\bar A = U\bar D U^{-1}$.

Much like the pairwise-squared deviations of points formulation of the $k$-means clustering problem, we can now write our problem as:
$$\arg\max_S \sum_{i=1}^k \frac{1}{2|S_i|} \sum_{\substack{x_i \neq x_j \\ x_j,x_i\in S_i}} \bar A_{ij}^2 $$

This problem can be solved with Algorithm \ref{kinvsub}.

\begin{figure}[t!]
	\begin{algorithm}[H]
		\caption{$k$-Invariant Subspaces Algorithm}
		\label{kinvsub}
		\begin{algorithmic}
			\WHILE{clusters have changed}   
			\FOR{each center $i$}
			\STATE {Find cluster $k$ which maximizes the score} $ \frac{1}{|S_k|} \left( \|\bar A_{i, S_k\setminus \{i\} } \|^2 + \|\bar A_{S_k\setminus \{i\},i} \|^2 \right)$
			\STATE Reassign center $i$ to cluster with highest score
			\ENDFOR
			\ENDWHILE   
			\STATE \textbf{return} clusters
		\end{algorithmic}
	\end{algorithm}
	\vspace{-0.2in}
\end{figure}



%\begin{figure}[t!]
%\begin{algorithm}
%
%	\begin{algorithmic}
%	%	\WHILE{clusters have changed}
%%		\FOR{each center $i$}
%%		%\STATE {find cluster $k$ which maximizes the score} $ \frac{1}{|S_k|} \left( \|\bar A_{i, S_k\setminus \{i\} } \|^2 + \|\bar A_{S_k\setminus \{i\},i} \|^2 \right)$
%%		\STATE reassign center $i$ to cluster with highest score
%%		\ENDFOR
%%		\ENDWHILE
%%		\STATE \textbf{return} clusters
%	\end{algorithmic}
%\end{algorithm}
%\end{figure}

Note the differences between this and the $k$-means clustering algorithm: first, we are \emph{maximizing} since the terms represent influence rather than distance. Secondly, in the possible case that $|S_k|=0$, we make the total value zero in order to avoid division by zero. Thirdly, we exclude centers' influence on themselves from the score by taking $S_k\setminus\{i\}$.

\rr{This algorithm can be repeated as many times as desired with different random initial conditions and different $k$-values in order to find the maximal configuration. These semi-invariant subspaces may then be used in methods such as Algorithm \ref{alg:samples} which depend on such a decomposition.}



\subsection{Generalizing Across Similar Spatiotemporally Evolving Systems} \label{sec:egp}
Building on the Kernel Observers method, let us introduce Evolving Gaussian Processes (E-GP). The primary novelty in this method of generating a model is learning an $\dualopApprox$ matrix for \emph{multiple} systems. The ultimate goal of this research would be to generate highly efficient machine learning models that can be used instead of the costly numerical simulations for design and autonomy purposes. This would be a major success for the design and control of complex physical systems, such as soft robotics, as they would significantly reduce the cost and resources required in simulations. The ability to generalize across different physical situations, is critical. This is a difficult problem, as it requires that the model have the capability to actually learn the underlying physics and not just input-output relationships. For example, in the context of fluid flows, these models must be able to predict fluid dynamics at different conditions (e.g. Reynolds number) than the training data. E-GP, as far as the authors know, was the first machine learning method to generalize across spatiotemporally evolving systems of such complexity using end-to-end data.

We found that the class of functional evolutions $\mathbb{F}$ defined by linear Markovian transitions in a RKHS is still sufficient to model the nonlinear Navier Stokes equations which govern fluid dynamics, since the unknown map $\fmap$ allows us to model highly nonlinear dynamics in the input space. However, we do expect that phenomena such as bifurcation or turbulence will require nonlinear mappings $\fspace$. There are three steps to generate an E-GP model:

\begin{enumerate}
	\item After picking the kernel and estimating the bandwidth hyperparameter $\s$ (we utilize the maximum likelihood approach, although other approaches can be used), find an optimal basis vector set $\shCent$ using the algorithm in \cite{csato2002sparse}.
	\item Use Gaussian process inference to find weight vectors for each time-step in the training set(s), generating the sequence $\weight_\tindex, \tindex=1,\dots,T$ for each system. A uniform time-step makes next step easier but can be worked around for non-uniform data sets
	\item Using the weight trajectory, use matrix least-squares with the equation $\dualopApprox [\weight_1,\weight_2, ...,\weight_{T-1}] = [\weight_2,\weight_3,...,\weight_T]$ to solve for $\dualopApprox$.
	\item To generate a multi-system model, concatenate the weight trajectories from each similar system in the least-squares computation of $\dualopApprox$. That is, let $W_{\theta} = [\weight_1^{(\theta)},\weight_2^{(\theta)}, ...,\weight_{n-1}^{(\theta)}]$ and $W_{\theta}' = [\weight_2^{(\theta)},\weight_3^{(\theta)}, ...,\weight_n^{(\theta)}]$ be the weight trajectory and next weight trajectory for some parameter . Then we solve the least-squares problem $\dualopApprox = [W_{\theta_1},\dots,W_{\theta_n}] = [W_{\theta}',\dots,W_{\theta_n}']$
\end{enumerate}

For the sake of defining when it is appropriate to expect this method to be able to generalize across different spatiotemporally evolving systems, we shall define what it means for two fluid flows to be \emph{similar}. In configuring a fluid dynamics simulation, a set of quantifiable parameters are defined. Two dynamical fluid systems $S_1$ and $S_2$ are considered \emph{similar} if they have  the same configuration of parameters and differ only in the value of at most one parameter. Furthermore, we require that the parameter be continuously variable, and that any observable quantity in the domain of the system vary smoothly as that parameter varies from its value in $S_1$ to its value in $S_2$. For example, for fluids flowing past identical cylinders, the Reynolds number associated with the free stream velocity may be varied to produce similar systems. However, to replace the system's cylinder with a triangle would be to qualitatively change the configuration of the system parameters, and thus would produce a non-similar system.

Unlike neural networks, the weights in an E-GP do not exist in some abstract, difficult-to-comprehend space, but are associated with kernel centers in specific locations in the domain. We refer to this attribute of E-GPs as the \emph{spatial encoding} property. This property is an extremely valuable tool for gaining insight into the learned model works:
\begin{enumerate}
	\item By plotting which kernel centers are associated with which invariant subspaces in the transition matrix, one can visualize where the eigenfunctions are found and how the dynamic modes are separated spatially.
	\item By plotting arrows from center $c_j$ to $c_i$ for each of the largest elements $\hat a_{ij}$ of $\dualopApprox$, one can visualize how different areas of the domain influence each other's evolution.
	\item By performing an eigendecomposition of the $\dualopApprox$ matrix, and transforming the eigenvectors back from the weight space to the function space, one can obtain the Koopman modes (and associated eigenvalues) of the system (see next section).
\end{enumerate}

\rr{Demonstration of E-GP's ability to generalize over similar dynamical systems may be found in the sidebar on Fluid Flows.}



\subsection{\rr{Spectral Analysis of Evolving Gaussian Process Model and Koopman Operator Theory}}

%Within the computational flow dynamics (CFD) community, there has recently been considerable interest in methods inspired by Koopman operator theory. A Koopman operator is a linear but infinite-dimensional operator that is defined for an autonomous dynamical system and governs the evolution of its \emph{observables} \cite{williams2015koopman}, rather than states. An observable is a function from the state space to a measurement space; the Koopman operator is in essence a composition operator on an observable and the state-transition operator, returning a new function which takes a state and gives a prediction of future measurements. By performing a spectral analysis of this linear operator, one reveals modes which show the spatial distribution, oscillation frequency, and growth rate/decay of the component dynamics of the system. Many advantages can be realized from this: the ability to transform the state space so the dynamics appear linear, to predict the temporal evolution of the linear system, to reconstruct the state of the original nonlinear system, and even to implement controller design. Dynamic Mode Decomposition (DMD) is the most widely used method for finding a finite-dimensional subspace of the Koopman operator's infinite-dimensional domain to work in. Williams et al., recently integrated DMD with the kernel trick, allowing the algorithm to be extended to systems with much larger dimensions \cite{williams2015kerneldmd}. Brunton et al., inspired by DMD, were able to generate governing equations from data by sparse identification of nonlinear dynamical systems \cite{brunton2016discovering}. However, these methods are restricted to approximating the Koopman operator given a fixed vector-valued observable, and have no way of effectively using measurements that vary in both size and location over time. Furthermore, the state of research into data-driven generalizing over similar systems with varying parameters is at best preliminary.

We would be remiss not to examine our methods in light of Koopman operator theory, which has served as a major source of inspiration for number of new methods in analyzing dynamical systems in the last decade. \rr{A Koopman operator is a linear but infinite-dimensional operator that is defined for an autonomous dynamical system and governs the evolution of its \emph{observables} \cite{williams2015koopman}, rather than states. An observable is a function from the state space to a measurement space; the Koopman operator essentially composes an observable with the state-transition operator, returning a new function which takes a state and gives a prediction of future measurements. The spectral analysis of this Koopman operator yields the major insights in this field.}

In the following proofs we present results which show a direct connection between the Koopman modes, eigenfunctions, and eigenvalues and the spectral decomposition of the transition matrix in our model. \rr{In fact, in our final theorem, we show that the eigenvalues of our model are all eigenvalues of the Koopman operator, the eigenvectors transformed to the input space are congruent in shape to the Koopman modes, and the eigenfunctions are identical}. These results have allowed us to construct new methods for sensor placement under even more difficult conditions than described previously -- specifically, the situation of one (or a few) sensors attached to moving agents or robots.

For a general dynamical system $f_{\tindex+1} = \mathbb{F}(f_{\tindex})$ defined on a state space (for us, an RKHS, i.e. $f\in\fspace$), we can define an arbitrary, vector-valued \emph{observable} $\measop : \fspace \to \R^{\nsamp}$ \rr{(as before, though with no requirement of linearity this time)}. Note that the space of observables $\measop \in \mathcal G$ is a vector space. The Koopman operator $U : \mathcal G \to \mathcal G$ is defined to be the operator on the space of observables such that:
\begin{align} \eqlabel{koopman_operator}
(U \measop)(f_{\tindex}) \coloneqq \measop(\mathbb{F}(f_{\tindex})) = \measop(f_{\tindex+1}) = y_{\tindex+1}
\end{align}
This operator is clearly linear from its definition, and thus it is reasonable to examine its spectral properties. The special observables $\phi : \fspace \to \C$ that have the property,
\begin{align} \eqlabel{koopman_eigenfunctions}
U \phi(f_{\tindex}) = \phi(\mathbb{F}(f_{\tindex})) = \phi(f_{\tindex+1}) = \lambda \phi(f_{\tindex})
\end{align}
are the eigenfunctions of $U$, and the associated $\lambda$ are the eigenvalues. \rr{As of yet, we have not involved the concept of space anywhere in our math.} So, suppose now that we have a vector-valued observable $\measop_x(\fspaceEl)$, where $x\in\dom$ is the measurement location and $\fspaceEl\in\fspace$. 

\begin{definition}
	The Koopman mode $s(x)$ at isolated eigenvalue $\lambda$ of algebraic multiplicity 1 is the projection of $\measop_x(f)$ onto the eigenfunction $\phi_{\lambda}(\fspaceEl)$ of $U$ at $\lambda$. \cite{mezic2013analysis}
\end{definition}

As previously shown by Rowley in 2009, the modes produced by the Dynamic Mode Decomposition (DMD) algorithm constitute a subset of Koopman modes \cite{rowley2009spectral}. Mezic showed that there exists, in principle if not in practice, a rigorous method for computing the full set of Koopman modes by a method known as generalized Laplace analysis (GLA) \cite{mezic2013analysis}. We have generated a number of results interpreting the E-GP model in terms of Koopman operator theory, culminating in the proof that the eigenvalues and eigenvectors of $\dualopApprox$ are related to the Koopman eigenvalues and modes.

\begin{proposition}\label{thm:ApproxObsv}
	Let $\fspace$ be a RKHS with  an approximate feature space $\fspaceApprox$, and let $\measop_x(\fspaceEl)$ be an observable in the Koopman sense with respect to the dynamical system $f_{\tindex+1} = \mathbb{F}(f_{\tindex})$ in $\fspace$. Then $\hat \measop_x(\fspaceEl) \coloneqq u(\fspaceApproxEl,x)$, where $\fspaceApproxEl$ is the projection of $\fspaceEl\in\fspace$ onto $\fspaceApprox$, is also an observable.
	%Given a RKHS $\fspace$ and an approximate feature space $\fspaceApprox$ which has finite dimension $\ncent$, the projection $\fmapApprox : \fspace \to \R^\ncent$ from the RKHS to the dual space of $\fspaceApprox$ is an observable in the Koopman sense with respect to the dynamical system $f_{\tindex+1} = \mathbb{F}(f_{\tindex})$
\end{proposition}


This follows from the fact that the projection from the function space to its subspace is well-defined when the $\fmapApprox_i$ are independent. Now, Koopman modes of observables are of interest because they are akin to the eigenvector expansions utilized in linear dynamics. If we separate the Koopman operator into $U = U_s + U_r$ where $U_s$ has a pure point spectrum and $U_r$ has a pure continuous spectrum, then we can write,
\begin{align} \eqlabel{spectral_expansion}
(U^t \measop_x)(\fspaceEl) = \measop^{\ast}(x) + \sum_{j=1}^k \lambda_j^t \phi_j(\fspaceEl) s_j (x)  +  U_r^t \measop_x(\fspaceEl)
\end{align}
where $\measop_x^{\ast}$ represents the time-averaged of part of the field, which corresponds with $\lambda=1$ (see GLA in \cite{mezic2013analysis}). The continuous-spectrum component is usually discarded/neglected since it represents the part of the field that is genuinely aperiodic (or chaotic) in time, which could be modeled as a stochastic process \cite{mezic2013analysis}. Now, from this expansion we can prove that:

\begin{proposition}\label{prop:ApproxModes}
	Let $\measop_x(\fspaceEl)$ and $\hat \measop_x(\fspaceEl)$ be observables as in Proposition \ref{thm:ApproxObsv}, and $s_j(x)$ be the Koopman modes associated with the projection of the former onto the eigenfunctions $\phi_j$ of $U$ at $\lambda_j$. Then the Koopman modes associated with $\hat \measop_x$ are:
	\begin{align}
	\hat s_j(x) = \frac{\phi_j(\fspaceApproxEl)}{\phi_j(\fspaceEl)} s_j(x)
	\end{align}
\end{proposition}
\begin{proof}
	Using the spectral expansion and the definition of the observables, we have:
	$$U^t \hat \measop_x(\fspaceEl) = U^t \measop_x(\fspaceApproxEl) $$
	$$ \hat \measop_x^{\ast} + \sum_{j=1}^k \lambda_j^t \phi_j(\fspaceEl) \hat s_j (x)  +  U_r^t \hat \measop_x(\fspaceEl) = \measop_x^{\ast} + \sum_{j=1}^k \lambda_j^t \phi_j(\fspaceApproxEl) s_j (x)  +  U_r^t \measop_x(\fspaceApproxEl)$$
	Since this must be true for all $t$, the terms must match, and the hypothesis follows.% see that this must be the spectral expansion of $\hat u(\fspaceEl,x)$
\end{proof}


Note that this means the modes of the two observables are identical in shape (only differing by a multiplicative constant). In the exceptional case that $\phi_j(\fspaceEl)=0$, the GLA method is unable to compute $s_j (x)$ anyway, so that is not an issue for this proposition. The final step in our analysis is to connect the Koopman modes with the spectral decomposition of  our model's $\dualopApprox$ operator in the dual space.

\begin{theorem}\label{thm:DualApproxModes}
	In an E-GP or KO model of the dynamical system $ f_{\tindex+1} = \sysop f_{\tindex}$,
	\begin{enumerate}
		\item The eigenvalues of $\dualopApprox$ are a subset of the eigenvalues of the Koopman operator.
		\item The eigenfunctions of the Koopman operator in the dual space correspond with a subset of the eigenfunctions of the Koopman operator on $\fspaceEl_0$, i.e. $\hat\phi_j(\weight_{\tindex}) = \phi_j(f_{\tindex})$.
		\item If the observable $\measop_x(\fspaceEl)$ is the evaluation operator, $\measop_x(\fspaceEl) \coloneqq \fspaceEl(x)$, then the eigenvectors $v_j$ of $\dualopApprox$ are related to the Koopman modes of the observable by:
		\begin{align}
			s_j (x) = \frac{\phi_j(\fspaceEl)}{\phi_j(\fspaceApproxEl)} \obsMatRow(x) \cdot v_j
		\end{align}
		where $\obsMatRow(x)$ is the feature map of our model as a row vector (see def. of $\obsMat$ from \eqref{k_measure}).
	\end{enumerate} 
\end{theorem}
\begin{proof}
	It should be clear that, as long as the projection onto the approximate feature space is well defined, that $P(f_{\tindex}) \coloneqq \weight_{\tindex} $ is an observable. Then if we let $\phi_j(\fspaceEl) = \langle P(\fspaceEl), q_j \rangle$, where $q_j$ are eigenvectors of the adjoint $\dualopApprox^{\ast}$ (that is, $\dualopApprox^{\ast} q_j = \bar \lambda_j q_j$), then $\phi_j$ is an eigenfunction of the Koopman operator in $\fspace$ since $U \phi_j(f_{\tindex}) = \langle P(f_{\tindex+1}), q_j \rangle = \langle \weight_{\tindex+1}, q_j \rangle = \langle \dualopApprox \weight_{\tindex}, q_j \rangle =  \langle \weight_{\tindex}, \dualopApprox^{\ast} q_j \rangle =  \lambda_j \langle \weight_{\tindex}, q_j \rangle =  \lambda_j \phi_j(f_{\tindex})$. This shows that:
	\rr{\begin{enumerate}
		\item The eigenvalues of $\dualopApprox$ correspond with that of the Koopman operator.
		\item The eigenfunctions of the Koopman operator in the dual space, known to be $\hat\phi_j(\cdot) = \langle \cdot, q_j \rangle$, correspond with the eigenfunctions of the Koopman operator in $\fspace$.
	\end{enumerate}}
	In the case that $\measop_x(\fspaceEl) \coloneqq \fspaceEl(x)$, we have the relationship $\hat \measop_x(\fspaceEl) = \fspaceApproxEl(x) = \obsMatRow(x) \cdot \weight(\fspaceEl)$ according to the formulation of our model. Now, it is well known that the spectral expansion of a finite linear system is:
	$$ \weight_t = \sum_{j=1}^{\ncent} \lambda_j^t \hat\phi_j(\weight_0) v_j$$
	where $v_j$ are the eigenvectors of the transition matrix. Therefore, we have:
	$$ U^t \hat \measop_x(\fspaceEl_0) = U^t \left( \obsMatRow(x) \cdot \weight(\fspaceEl_0) \right) = \obsMatRow(x) \cdot \weight_t = \obsMatRow(x) \cdot \left( \sum_{j=1}^{\ncent} \lambda_j \hat\phi_j(\weight_0)  v_j \right) = \sum_{j=1}^{\ncent} \lambda_j \phi_j(\fspaceEl_0) \obsMatRow(x) \cdot v_j$$
	
	Comparing this to the spectral expansion of $\hat \measop_x(\fspaceEl)$, we see (letting $\lambda_1=1$) that:
	\rr{\begin{enumerate}
		\setcounter{enumi}{2}
		\item First $\hat \measop_x^{\ast} = \obsMatRow(x) \cdot v_1$, second $\hat s_j(x) = \obsMatRow(x) \cdot v_j$, and thirdly there is no continuous spectrum component. The hypothesis follows from Proposition \ref{prop:ApproxModes}.
	\end{enumerate}}
	
\end{proof}

In the end, we have established a direct connection between the spectral decomposition of the transition matrix in the dual space of the approximate feature space and the Koopman modes, eigenfunctions, and eigenvalues. 

\begin{corollary}\label{cor:ModesApprox}
	Suppose, given an $\e>0$, we are able to find an approximate feature space $\fspaceApprox$ such that for all $\fspaceEl\in\fspace$ there exists $\fspaceApproxEl\in\fspaceApprox$ such that $\|\fspaceEl - \fspaceApproxEl\| < \e$. Then, if $\phi_j(\fspaceEl) \neq 0$, there exists an approximate feature space such that $\obsMatRow(x) \cdot v_j$ is arbitrarily close to the Koopman mode $s_j(x)$ (where $v_j$ is an eigenvector of the dual space transition matrix $\dualopApprox$).
\end{corollary}
\begin{proof}
	Since $\phi_j$ is continuous, we can make $|\phi_j(\fspaceEl) - \phi_j(\fspaceApproxEl)|$ arbitrarily small, which means if $\phi_j{\fspaceEl} \neq 0$ we can make $\frac{\phi_j(\fspaceEl)}{\phi_j(\fspaceApproxEl)}$ arbitrarily close to 1. From Theorem \ref{thm:DualApproxModes} and the fact that $s_j\in\fspace$ means $s_j$ is bounded, we can conclude that $\obsMatRow(x) \cdot v_j$ can be made arbitrarily close to $s_j(x)$ everywhere in $\dom$. %choose $\delta$ such that $\|\fspaceEl - \fspaceApproxEl\| < \delta$ implies $|\phi_j(\fspaceEl) - \phi_j(\fspaceApproxEl)| < 	% neeed to actually say why the phi are continuous
\end{proof}

In essence, this says that if we can find a close-enough approximate feature space, then the eigenvectors of the transition matrix in the dual space correspond exactly with the Koopman modes in the input space. \rr{There are many exciting possibilities that are opened up for us from this ability to find Koopman modes using the E-GP model, but for the purposes of this paper, the Koopman modes serve as a visual representation of the various dynamic structures present in the data.}


%\begin{corollary}
	%Suppose that for an approximate feature space $\fspaceApprox$, there exists $\e>0$ such that for all $\fspaceEl\in\fspace$ there exists $\fspaceApproxEl\in\fspaceApprox$ such that $\|\fspaceEl - \fspaceApproxEl\| < \e$. For all $\phi_j(\fspaceEl) \gg \e$, and if the phi are lipshitz, then $s_j(x) \approx \obsMatRow(x) \cdot v_j$
%\end{corollary}



% INVARIANT SUBSPACES SECTION HAS BEEN MOVED



%\rr{
%\subsubsection{Scoring Paths for a Moving Agent}
%
%Consider the problem of predicting the current state of a spatiotemporally-evolving function with a set of robots, each one of whom can only observe the phenomena partially. When there are sufficient such robots to cover the entire domain, there are many approaches in the coverage literature available for solving this problem \cite{hussein2006effective,hussein2007effective}. In those approaches, the robots figure out the best trajectories to create a network of connected sensors that can simultaneously observe the whole environment. However, when the number of robots is limited, full coverage of the domain is not possible. How a few robots whose combined sensors can only observe a small part of the domain can predict the state of the spatiotemporally-evolving function remains an open problem.  Here we consider a limiting case of this problem where only a single robot is available to observe the whole environment, but the robot has available to it an approximate model of the function learned through past experience Evolving Gaussian Processes.
%
%We will be using a Kalman filter in the weight space of the E-GP as described in previous sections. In order to determine where a moving agent should go in order to gather the measurements which will help it converge to the true state of the system fastest, we use information from the spectral analysis of the E-GP model. We postulate that the value of taking a measurement at a point in the domain with respect to a particular eigenvector is proportional to the following factors:
%
%\begin{enumerate}
%	\item The spatial extent of the eigenvector. This can be taken to be equivalent to the area under the curve of the magnitude of the complex function corresponding to that eigenvector (normalized by the function peak)
%	$$ SE_i = \frac{1}{\sup_x |v_i \hat\Phi(x)|} \int_X |v_i \hat\Phi(x)| dx $$
%	This Riemann integral can of course be approximated by discretely partitioning the space. From this point of view, we can see that this expression represents what proportion of the domain is subject to variance due to the oscillation of this mode.
%	
%	\item The expected size of the measurement itself, which we take to be proportional to $|v_i \hat \Phi(x)|$. This is included because greater measurements mean relatively greater correction to the state estimate.
%	
%	\item The ratio of the frequency with which that Koopman mode is visited to the frequency of its eigenvalue. This factor is included due to the aliasing effect, and is related to the concept of the Nyquist frequency. Modes with higher frequencies need to be visited more often than modes with lower frequencies.
%	
%	\item A discount factor equal to a decaying exponential of the number of times that eigenmode has been visited. This factor is included due to the exponential convergence of a Kernel Observer. The information gained by taking a measurement within the active area of a particular dynamic mode decreases exponentially the more measurements are taken.
%\end{enumerate}
%
%This scoring method allows one to decide the likeliest candidate amongst a large family of paths.
%
%}
%
%
%\rr{
%\subsubsection{Generating High-Scoring Random Paths}
%
%Leveraging the above insights, the following procedure can be utilized for high-scoring paths:
%\begin{enumerate}
%	\item Cluster the centers into invariant subspaces according to the $k$-invariant subspaces algorithm, with the best reasonable guess(es) for $k$. This is necessary in order that the robot may be sure it is keeping track of each of the independent subsystems.
%	\item Within each cluster, select a waypoints at local maxima of the sum of the Koopman modes. These are where the best measurements may be taken.
%	\item Generate a path by selecting waypoints randomly according to a weighting scheme. The weights are equal to the spatial extent of the clusters, multiplied by an exponential decay factor for the number of times each has been visited (see item 4 in the previous section)
%\end{enumerate}
% }
