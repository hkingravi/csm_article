\vspace{-0.1in}
\section{Kernel Observers}\label{sec:observers}
This section outlines our modeling framework and presents theoretical results associated with the number of sampling locations required for monitoring functional evolution. 
\input{sec_preliminaries}
\vspace{-0.1in}
\subsection{Main Results}\label{sec:theory_results}
%\vspace{-0.1in}
In this section, we prove results concerning the observability of spatiotemporally varying functions modeled by the functional evolution and measurement equations \eqref{k_measure} formulated in Section \ref{sec:formulation}. In particular,  observability of the system states implies that we can recover the current state of the spatiotemporally varying function using a small number of sampling locations $\nsamp$, which allows us to 1) track the function, and 2) predict its evolution forward in time. We work with the approximation $\fspaceApprox\approx\fspace$: given $\ncent$ basis functions, this implies that the dual space of $\fspaceApprox$ is $\R^{\ncent}$.
Proposition \ref{prop:1} shows that if $\dualopApprox$ has a full-rank Jordan decomposition, the observation matrix $\obsMat$ meeting a condition called \emph{shadedness} (Definition \ref{def:shaded}) is sufficient for the system to be observable. Proposition \ref{prop:2} provides a lower bound on the number of sampling locations required for observability which holds for any $\dualopApprox$.  Proposition \ref{prop:3} constructively shows the existence of an abstract measurement map $\measmap$ achieving this lower bound. Since the measurement map does not have the structure of a kernel matrix, a slightly weaker sufficient condition for the observability of any $\dualopApprox$ is in Theorem \ref{thm:1}. Finally, since both $\empK$ and $\empKCD$ are kernel matrices generated from a shared kernel, these observability results translate directly into controllability results. Proofs of all claims are in the appendix. 

\begin{definition}\label{def:shaded}
\textbf{(Shaded Observation Matrix)} Given $\kernel:\dom\times\dom\to\R$ positive-definite on a domain $\dom$, let $\{\fmapApprox_1(x), \dots, \fmapApprox_{\ncent}(x)\}$ be the set of bases generating an approximate feature map $\fmapApprox:\dom\to\fspaceApprox$, and let
$\sampSet = \sampSetLong$ be the set of sampling (or sensing) locations, with each $x_i\in\dom$. 
Let $\obsMat\in\R^{\nsamp\times\ncent}$ be the observation matrix, where $ \obsMat_{ij} := \fmapApprox_j(x_i)$. For each
row $\obsMat_{(i)} := \left[\begin{smallmatrix}
  \fmapApprox_1(x_i) & \cdots & \fmapApprox_{\ncent}(x_i)
 \end{smallmatrix}\right]$, define the set 
$\Ind_{(i)} := \{\iota_1^{(i)},\iota_2^{(i)},\dots, \iota_{\ncent_i}^{(i)}\}$ to be the indices in the observation
matrix row $i$ which are nonzero. 
Then if 
%\begin{align}\eqlabel{shaded_cond}
$\bigcup_{i\in\{1,\dots,\nsamp\}} \Ind^{(i)} = \{1,2,\dots, \ncent\}$,
%\end{align}
we denote $\obsMat$ as a \emph{shaded observation matrix} (see Figure \ref{fig:shadeda}).
\end{definition}
This definition seems quite abstract, so the following remark considers a more concrete example.
\begin{remark}\label{rem:shaded}
 let $\fmapApprox$ be generated by the dictionary given by $\shCent = \shCentLong$, $c_i\in\dom$. Note that since $\fmapApprox_j(x_i) = \l\fmap(x_i), \fmap(c_j)\r_{\fspace} = \kernel(x_i,c_j)$, $\obsMat$ is the kernel matrix between $\sampSet$ and $\shCent$. For the kernel matrix to be shaded thus implies that there does not exist an atom $\fmap(c_j)$ such that the projections $\l\fmap(x_i),\fmap(c_j)\r_{\fspace}$ vanish for all $x_i$, $1\leq i\leq \nsamp$. Intuitively, the shadedness property requires that the sensor locations $x_i$ are privy to information propagating from every $c_j$. As an example, note that, in principle, for the Gaussian kernel, a single row generates a shaded kernel matrix\footnote{However, in this case, the matrix can have many entries that are extremely close to zero, and will probably be very ill-conditioned.}.  
\end{remark}
% With this definition in place, we can prove the following proposition, which shows that if $\dualopC$ has a full-rank Jordan decomposition, a shaded observationrf matrix is sufficient to guarantee observability. 
\begin{proposition}\label{prop:1}
Given $\kernel:\dom\times\dom\to\R$ positive-definite on a domain $\dom$, let $\{\fmapApprox_1(x), \dots, \fmapApprox_{\ncent}(x)\}$ be the set of bases generating an approximate feature map $\fmapApprox:\dom\to\fspaceApprox$, and let
$\sampSet = \sampSetLong$, $x_i\in\dom$. Consider the discrete linear system on $\fspaceApprox$ given by the evolution and measurement equations \eqref{k_measure}. Suppose that a full-rank Jordan decomposition of $\dualopApprox\in\R^{\ncent\times\ncent}$ of the form $\dualopApprox = \JorP\JorLa\JorP^{-1}$ exists, where $\JorLa = 
\left[\begin{smallmatrix}\JorLa_1 &\cdots & \JorLa_{\JorMul}\end{smallmatrix}\right]$,
and there are no repeated eigenvalues. Then, given a set of time instances  $\Tset = \{\tindex_1,\tindex_2,\dots,\tindex_{\otime}\}$, and a set of sampling locations $\sampSet=\sampSetLong$,
the system \eqref{k_measure} is observable if the observation matrix $\empK_{ij}$ is shaded according to Definition \ref{def:shaded},
% $\empK^D$, the row vector generated by summing the rows of $\empK$, has all nonzero entries, 
$\Tset$ has distinct values, and $|\Tset| \geq \ncent$.
\end{proposition}
% \begin{remark}
%  For the system corresponding to \ref{prop:1}, 
% \end{remark}
When the eigenvalues of the system matrix are repeated, it is not enough for $\empK$ to be shaded. 
% Intuitively, repeated eigenvalues correspond to coupled evolution\footnote{We use the term coupled evolution to denote dynamic evolution in which some eigen-modes evolve together \gXX{check rewrite}}, which requires an increased number of sensors to discern \gXX{what's the best way to explain this better? Without invariant subspaces it may not be clear to some why this matters, should we merge this with the discussion after the below prop?}. 
In the next proposition, we take a geometric approach and utilize the rational canonical form  of $\dualopApprox$ to obtain a lower bound on the number of sampling locations required. Let $\nevals$ be the number of unique eigenvalues of $\dualopApprox$, and let $\geomMult{\eval_i}$ denote the geometric multiplicity of eigenvalue $\eval_i$. Then the \emph{cyclic index} of $\dualopApprox$ is defined as $\minmeas = \max_{1\leq i\leq\nevals}{\geomMult{\eval_i}}$\cite{wonham1974linear} (see preliminary section \ref{sec_prelim} for details).
\begin{proposition}\label{prop:2}
 Suppose that the conditions in Proposition \ref{prop:1} hold, with the relaxation that
 the Jordan blocks $\left[\begin{smallmatrix}
                           \JorLa_1 &\cdots & \JorLa_{\JorMul}
                          \end{smallmatrix}\right]$ may have 
 repeated eigenvalues (i.e. $\exists \JorLa_i$ and $\JorLa_j$ s.t. $\eval_i = \eval_j$). 
 Then there exist kernels $\kernel(x,y)$ such that 
 the lower bound $\minmeas$ on the number of sampling locations $\nsamp$ is given by the cyclic index of $\dualopApprox$. In other words, the system in \eqref{k_measure} is observable if $ \nsamp \geq \ell$.
\end{proposition}
Section \ref{sec:discussion} gives a concrete example to build intuition regarding this lower bound. We now show how to construct a matrix $\measmap$ corresponding to the lower bound $\minmeas$.
% \begin{figure}[t!]
% 	\begin{algorithm}[H]
% 		\caption{Measurement Map $\measmap$}
% 		\label{alg:measmap}
% 		\begin{algorithmic}
% 			\begin{footnotesize} 
% 				\STATE {\bfseries Input:} $\dualopApprox\in\R^{\ncent\times\ncent}$
% 				\STATE Compute Rational canonical form, such that $ \FrobC  = \FrobP^{-1} \dualopApprox^T\FrobP$. Set $\FrobC_0:=\FrobC$, and
% 				$\ncent_0:=\ncent$. 
% 				%           Obtain subspaces 
% 				%           $\linspace_i\subset\R^{\ncent}$ s.t. $\R^{\ncent} = \linspace_1\oplus\cdots\oplus\linspace_{\minmeas}$.
% 				\FOR{$i=1$ {\bfseries to} $\minmeas$}
% 				\STATE Obtain MP $\minpoly_i(\eval)$ of $\FrobC_{i-1}$. 
% 				This returns associated indices $\mmapInd{i}\subset\{1,2,\dots,\ncent_{i-1}\}$. 
% 				%             \STATE Use $\mmapInd{i}$ to select submatrix $\widetilde{\FrobP}_i$, where an index $j\in\mmapInd{i}$ 
% 				%                    indicates selection of the row and column associated to $j$.
% 				\STATE Construct vector $\linvec_i \in \R^{\ncent}$ such that 
% 				$\minpolyv_{\linvec_i}(\eval)=\minpoly_{i}(\eval)$ .
% 				\STATE Use indices $\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}$ to select matrix $\FrobC_i$. Set 
% 				$\ncent_i:= |\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}|$
% 				\ENDFOR
% 				\STATE Compute $ \premeasmap = [\linvec_1^T, \linvec_2^T,...,\linvec_{\minmeas}^T]^T$
% 				\STATE {\bfseries Output:} $ \measmap =\premeasmap\FrobP^{-1}  $
% 			\end{footnotesize}
% 		\end{algorithmic}
% 	\end{algorithm} 
% 	\vspace{-0.2in}
% \end{figure}
\begin{proposition}\label{prop:3}
Given the conditions stated in Proposition \ref{prop:2}, it is possible to construct a measurement map $\measmap \in \R^{\minmeas\times\ncent}$ for the system given by \eqref{k_measure}, such that the pair $(\measmap, \dualopApprox)$ is observable.
\end{proposition}
The construction provided in the proof of Proposition \ref{prop:3} is utilized in Algorithm \ref{alg:measmap}, which uses the rational canonical structure of $\dualopApprox$ to generate a series of vectors $\linvec_i\in\R^{\ncent}$, whose iterations $\{\linvec_1,\dots,\dualopApprox^{\acycdeg_1-1}\linvec_1,\dots,\linvec_{\minmeas},\dots,\dualopApprox^{\acycdeg_{\minmeas}-1}\linvec_{\minmeas}\}$ generate a basis for $\R^{\ncent}$.
Unfortunately, the measurement map $\measmap$, being an abstract construction unrelated to the kernel, does not directly select $\sampSet$. We will show how to use the measurement map to guide a search for $\sampSet$ in Remark \ref{rem:1} (in Appendix). For now, we state a sufficient condition for observability of a general system. 
\begin{theorem}\label{thm:1}
 Suppose that the conditions in Proposition \ref{prop:1} hold, with the relaxation that
 the Jordan blocks $\begin{bmatrix}\JorLa_1 & &\cdots & \JorLa_{\JorMul}\end{bmatrix}$ may have 
 repeated eigenvalues. Let $\minmeas$ be the cyclic index of $\dualopApprox$.
 Define 
 \begin{align}\eqlabel{empKShadFull}
  \empKShadFull = \left[\begin{smallmatrix}
                    \empK^{{(1)}^T} & 
                    \cdots &
                    \empK^{{(\minmeas)}^T}
                  \end{smallmatrix}\right]^T
 \end{align}
 as the \emph{$\minmeas$-shaded matrix} (see Figure \ref{fig:shadedb}) which consists of $\minmeas$ shaded matrices with the property that any subset of
 $\minmeas$
 columns in the matrix are linearly independent from each
 other. Then system \eqref{k_measure} is observable if $\Tset$ has distinct values, and $|\Tset| \geq \ncent$.
\end{theorem}
While Theorem \ref{thm:1} is a quite general result, the condition that any $\minmeas$ columns of $\empKShadFull$ be linearly independent is a very stringent condition. 
One scenario where this condition can be met with minimal measurements is in the case when the feature map $\fmapApprox(x)$ is generated by a dictionary of atoms with the Gaussian RBF kernel evaluated at sampling locations $\sampSetLong$ according to \eqref{fmap_dict}, where $x_i\in\dom\subset\R^d$, and $x_i$ are sampled from a non-degenerate probability distribution on $\dom$ such as the uniform distribution. For a semi-deterministic approach, when the dynamics matrix $\dualopApprox$ is block-diagonal, we can utilize a simple heuristic:
\begin{remark}\label{rem:1}
 Let $\dom$ be compact, $\shCent = \shCentLong$, $c_i\in\dom$, and let the approximate feature map be defined by \eqref{fmap_dict}. Consider the system \eqref{k_measure} with $\dualopApprox=\JorLa$, and let $\Tset = \{0,1,\dots,\ncent-1\}$. Then the measurement map $\measmap$'s values lie in $\{0, 1\}$; in particular, each row $\measmap^{(j)}$, $j\in\{1,\dots,\minmeas\}$, corresponds to a subspace $\fsubspaceC{j}$, generated by a subset of centers $\shCent^{(j)}\subset\shCent$. Generate samples $x_i^{(j)}$ to create a kernel matrix $\empK^{(j)}$ that is shaded only with respect to centers $\shCent^{(j)}$. Once this is done, move on to the next subspace $\fsubspaceC{j+1}$. When all $\minmeas$ rows of $\measmap$ are accounted for, construct the matrix $\empKShadFull$ as in \eqref{empKShadFull}. Then the resulting system $(\empKShadFull, \dualopApprox)$ is observable. 
\end{remark}

%\begin{figure}
%\begin{algorithm}[H]
%\caption{Measurement Map $\measmap$}
%\label{alg:measmap}
%\begin{algorithmic}
% \begin{footnotesize} 
%   \STATE {\bfseries Input:} $\dualopApprox\in\R^{\ncent\times\ncent}$
%   \STATE Compute Jordan canonical form, such that $ \JorLa  = \JorP^{-1} \dualopApprox^T\JorP$. Set $\JorLa_0:=\JorLa$, and
%          $\ncent_0:=\ncent$. 
%          \FOR{$i=1$ {\bfseries to} $\minmeas$}
%            \STATE Obtain MP $\minpoly_i(\eval)$ of $\JorLa_{i-1}$. 
%                   This returns associated indices $\mmapInd{i}\subset\{1,2,\dots,\ncent_{i-1}\}$. 
%            \STATE Construct vector $\linvec_i \in \R^{\ncent}$ such that 
%                   $\minpolyv_{\linvec_i}(\eval)=\minpoly_{i}(\eval)$ .
%            \STATE Use indices $\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}$ to select matrix $\JorLa_i$. Set 
%                   $\ncent_i:= |\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}|$
%          \ENDFOR
%   \STATE Compute $ \premeasmap = [\linvec_1, \linvec_2,...,\linvec_{\minmeas}]^T$
%   \STATE {\bfseries Output:} $ \measmap =\premeasmap\JorP^{-1}  $
%\end{footnotesize}
%\end{algorithmic}
%\end{algorithm}
%\vspace{-0.2in} 
%\end{figure}
This heuristic is formalized in Algorithm 2 in the supplementary. Note that in practice, the matrix $\dualopApprox$ needs to be inferred from measurements of the process $f_{\tindex}$. If no assumptions are placed on $\dualopApprox$, it's clear that at least $\ncent$ sensors are required for the system identification phase. Future work will study the precise conditions under which system identification is possible with less than $\ncent$ sensors. 
\begin{figure*}[ht!]
\centering
\resizebox{1\textwidth}{!}{
\framebox[1.1\textwidth]{
 \begin{tikzpicture}[->,>=stealth',auto,node distance=1cm,
  thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
  \node[inner sep=0pt,label=below:{\tiny Physical 
                                  sampling locations},draw] (physical) at (-8,-0.3)
    {\includegraphics[width=0.13\textwidth,
                      height=0.08\textwidth]{figures/lab.png}};
  \node[inner sep=0pt,label=below:{\tiny Data locations},draw] (data) at (-4.25,-0.3)
    {\includegraphics[width=0.13\textwidth,
                      height=0.08\textwidth]{figures/locations.pdf}};
  \node[inner sep=0pt,label=below:{\tiny Functional inference 
                                   (for $\estsysop$)},draw] (inference) at (-0.5,-0.3)
    {\includegraphics[trim=2.4cm 2.6cm 3.4cm 1.5cm,clip,
                      width=0.13\textwidth,
                      height=0.08\textwidth]{figures/kernel_evol_cauchy.pdf}};    
  \node[inner sep=0pt,label={[align=center]
       below:{\tiny Sensor location selection after\\[-1.7\jot]
              \tiny basis decomposition ($\minmeas=3$)}},draw] 
       (sensors) at (3.25,-0.3)
       {\includegraphics[width=0.13\textwidth,
                         height=0.08\textwidth]{figures/locations_sensors.pdf}};                          
  \node[inner sep=0pt,label=below:{\tiny Physical sensor placement},draw] (placement) at (6.75,-0.3)
    {\includegraphics[width=0.13\textwidth,
                      height=0.08\textwidth]{figures/lab_sensors2.png}};                                                
  \path[every node/.style={font=\sffamily\small}]
    (physical) edge node [right] {} (data)
    (data) edge node [right] {} (inference)
    (inference) edge node [right] {} (sensors)
    (sensors) edge node [right] {} (placement);
\end{tikzpicture}
} % end framebox
} % end resizebox
\caption{
Overall description of how the kernel observer fits in the sensing framework. Physical 
locations are mapped to data locations, over which historical data is collected 
as a time series. Functional inference is performed over $\fspaceApprox$ to 
solve for $\estsysop$. The measurement operator $\empK$ is then computed 
(see Figure \ref{fig:sensplace}), leading to sensor placement. 
}\label{fig:overall_system}
\end{figure*}
\begin{figure*}[ht!]
  \begin{minipage}{\textwidth}
  \centering
  \begin{minipage}{0.47\textwidth}
  \resizebox{1\textwidth}{!}{
  \framebox[1.2\textwidth]{  
  \begin{tikzpicture}[scale=1.0, every node/.style={minimum size=1cm},on grid]    
	\begin{scope}[scale=\diagscale, xshift=-250, yshift=0]
		% The frame:
		\fill[white,fill opacity=.85] (0,0) rectangle (7,7); % Opacity
		\draw[black, thin] (0,0) rectangle (7,7); 
		 % Agents:
		\draw [fill=red]
			(4.7,2.7) circle (.2) % Firms
			(2,2) circle (.2) % Households
			(3.5,4.6) circle (.2); % Banks
		\fill[black]
			(0.5,6.5) node[right, scale=\diagtexttop] {\small \textbf{Compute cyclic index:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\boldsymbol\minmeas = \mathbf{2}$}
			(4.5,2.1) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
			(2.5,1.4) node[left, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$}
			;
        % draw lines representing possible choices of decomposition                
        \draw[thick](1.3,4.1) to (6.5,4.1);
        \draw[thick](2.2,0.5) to (6.5,5.4);
        \draw[thick](2.5,0.5) to (2.5,5.3);
	\end{scope} 
	\begin{scope}[scale=\diagscale, xshift=0, yshift=0]
		% The frame:
		\draw[black, thin] (0,0) rectangle (7,7); 
		% Agents:
		\draw [fill=blue]
			(4.7,2.7) circle (.2) % Firms
			(2,2) circle (.2); % Households
		\draw [fill=green]			
			(3.5,4.6) circle (.2); % Banks
		 % Labels:
		\fill[black]
		        (0.5,6.5) node[right, scale=\diagtexttop] {\small \textbf{Compute meas. map:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\boldsymbol\measmap =[\mathbf\linvec_1, 
			                        \mathbf\linvec_2]^T\boldsymbol\JorP^{-1}$}
% 			(4.5,2.1) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
% 			(2.5,1.4) node[left, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
% 			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$}
			;
	\end{scope} 
	\begin{scope}[scale=\diagscale, xshift=250, yshift=0]
		% The frame:
		\draw[black, thin] (0,0) rectangle (7,7); 
		% Agents:
		\draw [fill=blue, opacity=0.4]
			(4.7,2.7) circle (.2) 
			(2,2) circle (.2)
			;
		\draw [fill=blue]
			(3.2,2) rectangle (3.55,2.35)
			; 
			\node (one) at (3.37,1.5) {\scriptsize$ 1 $};
		\draw [fill=green, opacity=0.4]
			(3.5,4.6) circle (.2)
			;
		\draw [fill=green]			
			(3.0,4.0) rectangle (3.35,4.35)
			;
			\node (two) at (3.17,3.5) {\scriptsize$ 2 $};
		\fill[black]
		        (0.5,6.5) node[right, scale=\diagtexttop] {\small \textbf{Sensor loc. selection:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\mathbf{x_1, \ x_2\in}\boldsymbol\dom \boldsymbol\Rightarrow \empKShadFull$}
% 			(4.5,2.1) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
% 			(4.5,2.1) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{x_2})$}
% 			(2.5,1.4) node[left, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
% 			(1.4,4.7) node[right, scale=\diagtext]{$\boldsymbol\fmap(\mathbf{x_1})$}
% 			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$}
			;
	\end{scope}	
  \end{tikzpicture}
  } % end framebox
  } % end resizebox  
  \end{minipage}
  \begin{minipage}{0.47\textwidth}
  \resizebox{1\textwidth}{!}{
  \framebox[1.2\textwidth]{  
  \begin{tikzpicture}[scale=1.0, every node/.style={minimum size=1cm},on grid]  
	\begin{scope}[scale=\diagscale, xshift=-150, yshift=0]
		% The frame:
		\fill[white,fill opacity=.85] (0,0) rectangle (7,7); % Opacity
		\draw[black, thin] (0,0) rectangle (7,7); 
		 % Agents:
		\draw [fill=red]
			(4.7,2.7) circle (.2) % Firms
			(2,2) circle (.2) % Households
			(3.5,4.6) circle (.2); % Banks
		\fill[black]
			(0.5,6.5) node[right, scale=\diagtexttop]{\small \textbf{Compute cyclic index:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\boldsymbol\minmeas = \mathbf{2}$}
			(4.5,2.1) node[right,scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
			(2.5,1.4) node[left,scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$};
        % draw lines representing possible choices of decomposition                
        \draw[thick](1.3,4.1) to (6.5,4.1);
        \draw[thick](2.2,0.5) to (6.5,5.4);
        \draw[thick](2.5,0.5) to (2.5,5.3);
	\end{scope} 
	\begin{scope}[scale=\diagscale, xshift=150, yshift=0]
		% The frame:
		\draw[black, thin] (0,0) rectangle (7,7); 
		% Agents:
		\draw [fill=blue, opacity=0.4]
			(4.7,2.7) circle (.2) % Firms
			(2,2) circle (.2)
			;
		\draw [fill=blue]
			(1.17,1.1) rectangle (1.17+0.35,1.1+0.35)
			(5.0,1.8) rectangle (5.35,2.15)
                        ;
           \node (one) at (1.31,0.7) {\scriptsize$ 1 $};
           \node (two) at (5.17,1.3) {\scriptsize$ 2 $};
		\draw [fill=green, opacity=0.4]
			(3.5,4.6) circle (.2)
			;
		\draw [fill=green]
                        (3.0,4.0) rectangle (3.35,4.35)
			;
			\node (three) at (3.17,3.5) {\scriptsize$ 3 $};
		 % Labels:
		\fill[black]
		        (0.5,6.5) node[right, scale=\diagtexttop] {\small \textbf{Random sampling:}}
			(0.7,5.8) node[right, scale=\diagtexttop]
			{$\mathbf{x_1, x_2, x_3\in} \boldsymbol\dom \Rightarrow \empKShadFull$}
% 			(4.5,2.1) node[right,scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_3})$}
% 			(2.5,1.4) node[left,scale=\diagtext]{$\boldsymbol\fmap(\mathbf{c_2})$}
% 			(3.5,4.6) node [above, scale=\diagtext] {$\boldsymbol\fmap(\mathbf{c_1})$}
			;
	\end{scope} 
  \end{tikzpicture}
  } % end framebox
  } % end resizebox                      
  %\captionof{figure}{Sensor selection using random sampling} \label{fig:sselec_rand}
  \end{minipage} 
  \end{minipage} 
  \caption{
  Diagram demonstrating sensor placement using the measurement map or random sampling approaches. 
  The circles represent data locations associated to bases (e.g. $c_j\Leftrightarrow\fmap(c_j)$) 
  and the squares represent sensor locations (e.g. $x_i\Leftrightarrow\fmap(x_i)$) .
  The cyclic index ($\minmeas=2$) indicates how many possible couplings of bases exist, 
  which can be represented 
  as a choice of $\binom{\ncent}{\minmeas}$ hyperplanes in $\dom$. If the measurement map is 
  computed (left), the correct couplings are chosen (green vs. blue), and a smaller number of sensors (2) can be placed.
  Alternatively, random sampling (right) is more computationally efficient, but generally
  requires more sensors (3). 
  }\label{fig:sensplace}
\end{figure*}
\subsection{Discussion of Theoretical Results}\label{sec:discussion}
The systems-theoretic approach taken in this paper reveals something rather surprising: functions with complex dynamics (with a small cyclic index) can be recovered with less sensor placements than functions with simpler dynamics. Although seemingly counterintuitive, it becomes clear that this is because complex dynamics, which are characterized by a lower geometric multiplicity of the eigenvalues, ensure that the orbit $\orbit := \{\dualopApprox\weight_{\tindex}\}_{\tindex\in\Tset}$ traverses a greater portion of $\R^{\ncent} \equiv \fspaceApprox$ and thus that fewer sensors can recover more geometric information. On the other hand, in `simpler' functional evolution, $\orbit$ evolves along strict subspaces of $\R^{\ncent}$, and so more independent sensors are required to infer the same amount of information. 

In the case described in Remark \ref{rem:1}, we have a set of centers $\shCent=\shCentLong$, which generate the bases $\Atoms = \{\fmap(c_1), \cdots , \fmap(c_{\ncent})\}$. Let the cyclic index be $\minmeas$: this implies that there exist $\minmeas$ subsets $\atomSubset{i}$ of $\Atoms$ with at least one element $\fmap(c_j)$ each, leading to $\binom{\ncent}{\minmeas}$ possible choices: Figure \ref{fig:sensplace} represents these choices as hyperplanes separating the subsets. 
The measurement map described in Alg. \ref{alg:measmap} induces this \emph{decomposition of bases} $\Atoms = \{\atomSubset{1},\dots,\atomSubset{\minmeas}\}$ in polynomial time. Further, each subset $\atomSubset{i}$ is directly associated to a subset of centers $\centerSubset{i}\subset\shCent$, which allows us to pick targeted sensor locations $x_i\in\dom$. In particular, for radially symmetric kernels such as the Gaussian, the centroid of the convex hull of $\centerSubset{i}$ is sufficient for generating a sensor placement. The measurement map is a significant theoretical insight into sensor placement for dynamically changing environments, because it directly takes into account the dynamics of the process. Of course, in practice, this may be too expensive for approximate feature spaces with $\ncent$ very large, so one can use random sampling to generate the sensor locations instead, at the cost of $\nsamp$ being larger than $\minmeas$. The advantage here though is that since random sampling is computationally inexpensive, different choices of sensor placements can be generated and evaluated relatively quickly.

Another point to note is that since the collection of bases $\{\fmapApprox_i(x)\}_{i=1}^{\ncent}$ determines the richness of the function space $\fspaceApprox\approx\fspace$ we operate in, it determines the fidelity of the model approximation to the true time-varying function. As a consequence, observability of the system in $\fspaceApprox$ refers to the best possible approximation in $\fspaceApprox$. The greater the number of bases, the higher the dimensionality, which results in greater model fidelity, but which may require a much greater number of measurements for state recovery. This is where the lower bounds presented in the paper are particularly useful, because they show that for functional evolutions corresponding to certain $\dualopApprox$, \emph{the number of sensor placements are essentially independent of the dimensionality $\ncent$}, but depend rather on the cyclic index of $\dualopApprox$.

Figure \ref{fig:overall_system} gives an overall picture on the process of generating a kernel observer, while Figure \ref{fig:sensplace} gives two approaches to sensor selection in our framework. The measurement map approach can generate a smaller set of sensors than the random placement approach, but comes at an additional computational cost. 

\subsection{Random Sensor Placement}\label{sec:random_results}
We now elaborate on how the challenging problem of sensor placement can be tackled through random selection. This process of random selection is a product of the kernel observer model described in the section \ref{sec:observers}. We present the theoretical background required to prove Theorem \ref{thm:r1}, which states the expected number of randomly placed sensors required to monitor a given spatiotemporal process, and Theorem \ref{thm:r2}, which determines the probability with which optimal sensor placement is ensured given that, $\nsamp$ number of sensors have been placed. 
%Otherwise to the best of our knowledge there does not exist any sensor placement design based on random selection.%Thus, the kernel observer is a modeling solution for spatiotemporal processes that also guides deterministic as well as random placement of sensors.

As discussed earlier, we work with an approximate feature space $ \fspaceApprox $, with the corresponding transition operator $ \dualopApprox: \fspaceApprox \rightarrow \fspaceApprox $, representing finite-dimensional functional evolution. To achieve observability for the pair ($ \dualopApprox , \empK $), row vectors of the corresponding observability matrix, $ \Obs $, should form the basis for the $ \R^\ncent $-dimensional space $ \fspaceApprox $. According to the rational canonical structure Theorem \cite{wonham1974linear}, $\dualopApprox$ can successively decompose the dual space $ \R^\ncent $ into subspaces, $\linspace_i \subset \linspace$, $i\in \{1,\dots,\minmeas\}$, with properties, i) $\linspace = \linspace_1 \oplus ... \oplus \linspace_{\minmeas}$, ii) $\dualopApprox\linspace_i \subset \linspace_i$, and iii) $\dualopApprox|\linspace_i, i \in \{1,\dots,\minmeas\}$, are cyclic. The integer $\minmeas$ is unique and is called the \emph{cyclic index of $\dualopApprox$}.  Each of these properties contribute towards the theorem on the number of random samples required to achieve observability. The first property shows that the space $ \R^\ncent $ can be decomposed into $ \ell $ independent subspaces. The second property shows that the vector $ \linvec_i \in \linspace_i $ stays in $ \linspace_i $ even when operated upon by $ \dualopApprox $. Thus, to generate bases for $ \R^\ncent $, one needs at least $ \ell $ vectors, say, $ \linvec_1, \dots,\linvec_\ell $, with respect to each subspace $ \linspace_1,\dots,\linspace_\ell $. This holds due to the third property, but requires that the vectors  $ \linvec_1, \dots,\linvec_\ell, $ are the cyclic generators of their corresponding subspaces. Our analysis is based on whether a randomly selected sensor can generate a cyclic generator. To examine this, recall that a row vector $ \empK_{(i)} $ generated by a randomly selected sensor location $ x_i $ takes the form,
\begin{equation}\label{eq:rowvec}
\empK_{(i)} = \bbm k(x_i,c_1),\dots,k(x_i,c_\ncent) \ebm.
\end{equation}
Here, for radial kernels for example, the entries corresponding to the centers closer to $ x_i $ tend to be non-zero, whereas the others tend to be zero. 
% 	Note this holds since in general kernel function depends upon the metric of distance between its argument, for example the squared exponential kernel. 
% 	This limits the capability of a cyclic generator being obtained from a random sensor, however the latter certainly generates a non-zero entry for a closest sensor data point. 
The rows $\empK_{(i)}$ from random sensor placement must be able to generate a basis for a subspace $ \linspace_i $, and thus must be cyclic generators.  We will derive the expected number of random sensor placements sufficient for observability for the case where $ \dualopApprox = \JorLa $ and then attempt to generalize the result for any $ \dualopApprox $. Note $ \JorLa $ is a block diagonal Jordan form.  In this case, the cyclic generator for each subspace $ 
\linspace_i $, is a vector $ \linvec_i $ with non-zero entries corresponding to the leading entry of the Jordan blocks of $ 
\linspace_i $. 
%An example of a subspace, and  its cyclic generator is, 
%\begin{align}
%\linspace_1 & = \begin{bmatrix}{
%1 & 1 & 0 & 0\\
%0 & 1 & 1 & 0\\
%0 & 0 & 1 & 0\\
%0 & 0 & 0 & 2}
%\end{bmatrix}, \quad  \linvec_1 = \begin{bmatrix}{
%0 \\
%0 \\
%s \\
%s'},
%\end{bmatrix},  \nonumber 
%\end{align}
%where $s,s'$ are non-zero. 

Overall, our construction is as follows: for each subspace $ \linspace_i $, let $ \shCent_{\linspace_i} \subset \shCent $ be the centers corresponding to those leading entry of Jordan blocks: then the minimum number of random samples required to generate the bases for $ \linspace_i $ is equal to the number of Jordan blocks comprising $ \linspace_i $. Altogether, the minimum number of random samples required to generate a basis for $ \R^\ncent $ is equal to the total number of Jordan blocks in $ \dualopApprox $. Let $ \rands $ be the total number of Jordan blocks in $ \dualopApprox  $, then
%	For entire space we obtain the measurement map $ \measmap = [\linvec_1^T, \linvec_2^T,...,\linvec_{\minmeas}^T]^T $.
\begin{equation}\label{rands}
\rands = \sum_{\lambda \in \sigma(\dualopApprox)} \gamma_{\dualopApprox}(\lambda) \qquad %\geomMult_\lambda
\end{equation}
where $\sigma(\dualopApprox) $ represents the spectrum of $ \dualopApprox $, whose elements are the eigenvalues of $ \dualopApprox $, and $ \gamma_{\dualopApprox}(\lambda) $ is the geometric multiplicity corresponding to the eigenvalue $ \lambda $, which is also equal to the total number of Jordan blocks corresponding to the eigenvalue $ \lambda $. Define a set of centers $ \shCent_\rands $ with elements $ \{c_1, c_2,\dots, c_\rands\} $, to be the centers corresponding to the leading entries of  the Jordan blocks.
For sensor location $x\in\dom$, and $ \epsilon > 0 $, let $\kernel(x, c_j) > \epsilon$, denote the region $\dom_j\subset\dom$, such that the kernel evaluation with respect to center $c_j$ is greater than $\e$, that is $ \dom_j \equiv \{x\in \dom:\kernel(x, c_j) > \epsilon\} $. We define
$ p_{\e} $ as
\begin{equation}\label{ppp}
p_{\e} = \min_{c_j \in \shCent_\rands} \frac{\measure(\kernel(x, c_j) > \epsilon)}{\measure(\dom)},
\end{equation}
where $\measure$ is a measure in the real analysis sense. Hence, $p_{\e}$ corresponds to a lower bound on the probability that a random sample lies within the $ \epsilon-$shaded region of a particular center $ c_j$. With all of this in place, we can prove the following theorem. 
\begin{theorem}\label{thm:r1}
	Given the spatiotemporal function $ f(x,\tindex) $ with $ x \in \dom \subseteq  \mathbb{R}^\dimI, \tindex\in \mathbb{Z}^+  $ its kernel observer model \eqref{k_measure}, and a tolerance parameter $\e>0$, the expected number of randomly placed sensor locations required to achieve observability for the pair $ (\empK,\dualopApprox) $ is $ \rands/{p_{\e}} $ where $ \rands $ is the summation over geometric multiplicities of each $ \lambda \in \sigma(\dualopApprox) $  given by Equation (\ref{rands}).
\end{theorem}
\begin{theorem}\label{thm:r2}
	Given the spatiotemporal function $ f(x,\tindex) $ with $ x \in \dom \subseteq  \mathbb{R}^\dimI, \tindex\in \mathbb{Z}^+  $, its kernel observer model \eqref{k_measure}, a tolerance parameter $\e>0$, summation over geometric multiplicities of each $ \lambda \in \sigma(\dualopApprox) $ denoted by $ \rands  $ as in Equation (\ref{rands}), and a constant $ \delta \in (0,1] $, the probability that pair $ (\empK, \dualopApprox) $ is unobservable after the selection of $ \nsamp $ random sensors is at most $ e^{\frac{-1}{2}(\nsamp p_{\e}-2\rands)} $, where $ p_{\e} $ is given by Equation (\ref{ppp}) and $ \nsamp \geq \rands/p_{\e} $.
\end{theorem}
For the case when $ \dualopApprox \neq \JorLa $, a change of basis can be used to obtain $ \JorLa = P^{-1}\dualopApprox P $, where $ P $ is the projection map. There are two challenges in performing the above analysis for $\JorLa$ so obtained: first, the leading entries of Jordan blocks do not directly correspond to the centers $\{ c_1,\dots,c_\ncent\}$ which was the case for $ \dualopApprox = \JorLa $. Second, although we can obtain the transformation of the row vector (Equation (\ref{eq:rowvec})) using the projection map $P$, we can no longer arrive at the definition of the probability $p_{\e}$ as in Equation (\ref{ppp}). The existence of the similarity transform hints that the results in Theorems \ref{thm:r1}-\ref{thm:r2} should hold for any $ \dualopApprox$, but the mathematical tools utilized in the paper seem to be insufficient to prove them. However, we present some empirical evidence for these claims for when $ \dualopApprox \neq \JorLa $  in Section \ref{sec:exp}.


\subsection{Generalizing Across Similar Spatiotemporally Evolving Systems}
Building on the Kernel Observers method, let us introduce Evolving Gaussian Processes (E-GP). The primary novelty in this method of generating a model is learning an $\dualopApprox$ matrix for \emph{multiple} systems. The ultimate goal of this research would be to generate highly efficient machine learning models that can be used instead of the costly numerical simulations for design and autonomy purposes. This would be a major success for the design and control of complex physical systems, such as soft robotics, as they would significantly reduce the cost and resources required in simulations. The ability to generalize across different physical situations, is critical. This is a difficult problem, as it requires that the model have the capability to actually learn the underlying physics and not just input-output relationships. For example, in the context of fluid flows, these models must be able to predict fluid dynamics at different conditions (e.g. Reynolds number) than the training data. E-GP, as far as the authors know, was the first machine learning method to generalize across spatiotemporally evolving systems of such complexity using end-to-end data.

We found that the class of functional evolutions $\mathbb{F}$ defined by linear Markovian transitions in a RKHS is still sufficient to model the nonlinear Navier Stokes equations which govern fluid dynamics, since the unknown map $\fmap$ allows us to model highly nonlinear dynamics in the input space. However, we do expect that phenomena such as bifurcation or turbulence will require nonlinear mappings $\fspace$. There are three steps to generate an E-GP model:

\begin{enumerate}
	\item After picking the kernel and estimating the bandwidth hyperparameter $\s$ (we utilize the maximum likelihood approach, although other approaches can be used), find an optimal basis vector set $\shCent$ using the algorithm in \cite{csato2002sparse}.
	\item Use Gaussian process inference to find weight vectors for each time-step in the training set(s), generating the sequence $\weight_\tindex, \tindex=1,\dots,T$ for each system. A uniform time-step makes next step easier but can be worked around for non-uniform data sets
	\item Using the weight trajectory, use matrix least-squares with the equation $\dualopApprox [\weight_1,\weight_2, ...,\weight_{T-1}] = [\weight_2,\weight_3,...,\weight_T]$ to solve for $\dualopApprox$.
	\item To generate a multi-system model, concatenate the weight trajectories from each similar system in the least-squares computation of $\dualopApprox$. That is, let $W_{\theta} = [\weight_1^{(\theta)},\weight_2^{(\theta)}, ...,\weight_{n-1}^{(\theta)}]$ and $W_{\theta}' = [\weight_2^{(\theta)},\weight_3^{(\theta)}, ...,\weight_n^{(\theta)}]$ be the weight trajectory and next weight trajectory for some parameter . Then we solve the least-squares problem $\dualopApprox = [W_{\theta_1},\dots,W_{\theta_n}] = [W_{\theta}',\dots,W_{\theta_n}']$
\end{enumerate}

For the sake of defining when it is appropriate to expect this method to be able to generalize across different spatiotemporally evolving systems, we shall define what it means for two fluid flows to be \emph{similar}. In configuring a fluid dynamics simulation, a set of quantifiable parameters are defined. Two dynamical fluid systems $S_1$ and $S_2$ are considered \emph{similar} if they have  the same configuration of parameters and differ only in the value of at most one parameter. Furthermore, we require that the parameter be continuously variable, and that any observable quantity in the domain of the system vary smoothly as that parameter varies from its value in $S_1$ to its value in $S_2$. For example, for fluids flowing past identical cylinders, the Reynolds number associated with the free stream velocity may be varied to produce similar systems. However, to replace the system's cylinder with a triangle would be to qualitatively change the configuration of the system parameters, and thus would produce a non-similar system.

Unlike neural networks, the weights in an E-GP do not exist in some abstract, difficult-to-comprehend space, but are associated with kernel centers in specific locations in the domain. We refer to this attribute of E-GPs as the \emph{spatial encoding} property. This property is an extremely valuable tool for gaining insight into the learned model works:
\begin{enumerate}
	\item By plotting which kernel centers are associated with which invariant subspaces in the transition matrix, one can visualize where the eigenfunctions are found and how the dynamic modes are separated spatially
	\item By plotting arrows from center $c_j$ to $c_i$ for each of the largest elements $\hat a_{ij}$ of $\dualopApprox$, one can visualize how different areas of the domain influence each other's evolution.
	\item By performing an eigendecomposition of the $\dualopApprox$ matrix, and transforming the eigenvectors back from the weight space to the function space, one can obtain the Koopman modes (and associated eigenvalues) of the system.
\end{enumerate}
