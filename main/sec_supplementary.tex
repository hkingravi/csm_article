\section{Appendix}
This part of the paper presents extra details for the paper. 
\subsection{Proofs of Main Theorems}
% \begin{definition}\label{def:shaded}
% 	\textbf{(Shaded Observation Matrix)} Given $\kernel:\dom\times\dom\to\R$ positive-definite on a domain $\dom$, let $\{\fmapApprox_1(x), \dots, \fmapApprox_{\ncent}(x)\}$ be the set of bases generating an approximate feature map $\fmapApprox:\dom\to\fspaceApprox$, and let
% 	$\sampSet = \sampSetLong$, $x_i\in\dom$. 
% 	Let $\obsMat\in\R^{\nsamp\times\ncent}$ be the observation matrix, where $ \obsMat_{ij} := \fmapApprox_j(x_i)$. For each
% 	row $\obsMat_{(i)} := \left[\begin{smallmatrix}
% 	\fmapApprox_1(x_i) & \cdots & \fmapApprox_{\ncent}(x_i)
% 	\end{smallmatrix}\right]$, define the set 
% 	$\Ind_{(i)} := \{\iota_1^{(i)},\iota_2^{(i)},\dots, \iota_{\ncent_i}^{(i)}\}$ to be the indices in the observation
% 	matrix row $i$ which are nonzero. 
% 	Then if 
% 	%\begin{align}\eqlabel{shaded_cond}
% 	$\bigcup_{i\in\{1,\dots,\nsamp\}} \Ind^{(i)} = \{1,2,\dots, \ncent\}$,
% 	%\end{align}
% 	we denote $\obsMat$ as a \emph{shaded observation matrix} (see Figure \ref{fig:shadeda}).
% \end{definition}
% This definition seems quite abstract, so the following remark considers a more concrete example.
% \begin{remark}\label{rem:shaded}
% 	let $\fmapApprox$ be generated by the dictionary given by $\shCent = \shCentLong$, $c_i\in\dom$. Note that since $\fmapApprox_j(x_i) = \l\fmap(x_i), \fmap(c_j)\r_{\fspace} = \kernel(x_i,c_j)$, $\obsMat$ is the kernel matrix between $\sampSet$ and $\shCent$. For the kernel matrix to be shaded thus implies that there does not exist an atom $\fmap(c_j)$ such that the projections $\l\fmap(x_i),\fmap(c_j)\r_{\fspace}$ vanish for all $x_i$, $1\leq i\leq \nsamp$. Intuitively, the shadedness property requires that the sensor locations $x_i$ are privy to information propagating from every $c_j$. As an example, note that, in principle, for the Gaussian kernel, a single row generates a shaded kernel matrix\footnote{\tiny{However, in this case, the matrix can have many entries that are extremely close to zero, and will probably be very ill-conditioned.}}.  
% \end{remark}
% This condition implies that there does not exist an atom $\fmap(c_j)$ s.t. the projections $\l\fmap(x_i),\fmap(c_j)\r_{\fspaceApprox}$ vanish for all $x_i$, $1\leq i\leq \nsamp$. Intuitively, the shadedness property requires that the sensor locations $x_i$ are privy to information propagating from every atom $c_j$. As an example, note that, in principle, for the Gaussian kernel, a single row generates a shaded kernel matrix\footnote{\tiny{However, in this case, the matrix can have many entries that are extremely close to zero, and will probably be very ill-conditioned.}}. 
%With this definition in place, we can prove the following proposition, which shows that if $\dualopApprox$ has a full-rank Jordan decomposition, a shaded kernel matrix is sufficient to guarantee observability.


% Let $\kernel:\dom\times\dom\to\R$ be a positive definite kernel on a domain $\dom\subset\dom$. 
% Let $C = [c_1,  c_2, \cdots , c_{\ncent}\}$, $c_j\in\dom$ be the centers generating a 
% finite-dimensional covering of the reproducing kernel Hilbert space $\fspaceApprox$ associated to $\kernel(x,y)$, and
% consider the discrete linear system on $\fspaceApprox$ given by
% \begin{align}\eqlabel{hilbert_disc_trans}
%  w_{t+1} = Aw_{t},
% \end{align}
% where $A\in\R^{\ncent\times\ncent}$ has a Jordan decomposition of the form $A = \JorP\JorLa\JorP^{-1}$,
% where $\JorLa$ consists of Jordan blocks along the diagonal, i.e. 
% \begin{align*}
%  \JorLa = 
%  \begin{bmatrix}
%   \JorLa_1 & 0 & 0 &\cdots & 0 \\
%   0 & \JorLa_2 & 0 & \cdots & 0\\
%   \vdots & \vdots & \vdots & \ddots & \vdots\\
%   0 & 0 & 0 & \cdots & \JorLa_{\JorMul}
%  \end{bmatrix}, 
% \end{align*}
% where 
% \begin{align*}
% \JorLa_i := 
%  \begin{bmatrix}
%   \la_i & 1 & 0 & \cdots & 0 & 0\\
%    0 & \la_i & 1 & \cdots & 0 & 0\\
%    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
%    0 & 0 & 0 & \cdots & 0 & \la_i
%  \end{bmatrix}.
% \end{align*}
% Given 
% a set of time instances  $\Tset = \{\tindex_1,\tindex_2,\dots,\tindex_{\otime}\}$, and a set of sampling locations $\sampSet=\sampSetLong$,
% the system \eqref{hilbert_disc_trans} is observable if the kernel matrix $\empK_{ij} := \kernel(x_i,c_j)$ is shaded,
% $\empK^D$, the row vector generated by summing the rows of $\empK$, has all nonzero entries, 
% $\Tset$ has distinct values, and $|\Tset| \geq \ncent$.
Before we prove Proposition \ref{prop:1}, recall that a Jordan decomposition of a matrix $\dualopApprox\in\R^{\ncent\times\ncent}$ with no repeated eigenvalues can be composed of $\JorMul$ Jordan blocks, where $\JorMul$ can be strictly less than $\ncent$. This is in direct contrast to the case where the matrix $\dualopApprox$ has an eigenvalue decomposition, in which case no repeated eigenvalues implies that $\JorMul = \ncent$.  
% \begin{proposition}\label{prop:1}
% 	Given $\kernel:\dom\times\dom\to\R$ positive-definite on a domain $\dom$, let $\{\fmapApprox_1(x), \dots, \fmapApprox_{\ncent}(x)\}$ be the set of bases generating an approximate feature map $\fmapApprox:\dom\to\fspaceApprox$, and let
% 	$\sampSet = \sampSetLong$, $x_i\in\dom$. Consider the discrete linear system on $\fspaceApprox$ given by the evolution and measurement equations \eqref{k_measure}. Suppose that a full-rank Jordan decomposition of $\dualopApprox\in\R^{\ncent\times\ncent}$ of the form $\dualopApprox = \JorP\JorLa\JorP^{-1}$ exists, where $\JorLa = 
% 	\left[\begin{smallmatrix}\JorLa_1 &\cdots & \JorLa_{\JorMul}\end{smallmatrix}\right]$,
% 	and there are no repeated eigenvalues. Then, given a set of time instances  $\Tset = \{\tindex_1,\tindex_2,\dots,\tindex_{\otime}\}$, and a set of sampling locations $\sampSet=\sampSetLong$,
% 	the system \eqref{k_measure} is observable if the observation matrix $\empK_{ij}$ is shaded according to Definition \ref{def:shaded},
% 	% $\empK^D$, the row vector generated by summing the rows of $\empK$, has all nonzero entries, 
% 	$\Tset$ has distinct values, and $|\Tset| \geq \ncent$.
% \end{proposition}
\begin{proof}
 \textbf{(Proposition \ref{prop:1})}
 To begin, consider a system where $\dualopApprox = \JorLa$, with Jordan blocks $\{\JorLa_1, \JorLa_2, \dots, \JorLa_{\JorMul}\}$ along the 
 diagonal. Then $\dualopApprox^{\tindex_i} = \diag(\begin{bmatrix}\JorLa_1^{\tindex_i} & \JorLa_2^{\tindex_i} & \cdots & \JorLa_{\JorMul}^{\tindex_i}\end{bmatrix})$. 
 %The exponentiation of each Jordan block results in upper triangular matrices with linearly independent columns. 
 We have that 
 \begin{align*}
 \Obs_{\Tset} &=
 \underbrace{
 \begin{bmatrix} 
  \empK \dualopApprox^{\tindex_1}\\
  \cdots\\
  \empK \dualopApprox^{\tindex_\otime}.
 \end{bmatrix}}_{\Obs_{\Tset}\in\R^{\nsamp\otime\times\ncent}}
 \end{align*}
 We need to prove that the column rank of $\Obs_{\Tset}$ is $\ncent$, which is not immediately
 obvious since typically $\nsamp \ll \ncent$. To prove the statement, we will show that 
 computing the rank of $\Obs_{\Tset}$ is equivalent to the rank computation of the product of 
 two simple matrices. In what follows,
 we use the notation $\zerosMat{I}{J}$ to denote an $I\times J$ matrix of all zeros. 
 
 In the first step, we write the above matrix as the product of two matrices. 
 Then it can be 
 shown that $\Obs_{\Tset}$ is the product of two block matrices 
 \begin{align*}
 \Obs_{\Tset}  
 &=
 \underbrace{
 \begin{bmatrix}
  \empK   & \cdots & \zerosK\\
  \vdots  & \ddots & \vdots\\
  \zerosK  & \cdots & \empK
 \end{bmatrix}}_{\catempK\in\R^{\nsamp\otime\times\ncent\otime}}
 \underbrace{
 \begin{bmatrix}
  \JorLa_1^{\tindex_1} & \cdots & 0\\
   \vdots & \ddots & \vdots\\
  0 & \cdots & \JorLa_{\JorMul}^{\tindex_1}\\ 
   \hline
   \vdots & \ddots & \vdots\\
   \hline
  \JorLa_1^{\tindex_{\otime}} & \cdots & 0\\
   \vdots & \ddots & \vdots\\
  0 & \cdots & \JorLa_{\JorMul}^{\tindex_\otime}   
 \end{bmatrix}}_{\catdualopC\in\R^{\ncent\otime\times\ncent}}.
\end{align*}
We need to simplify $\catempK$ even further. 
Recall that a matrix's rank is preserved under a product with an invertible matrix. Design a 
matrix of elementary row operations $U\in\R^{\nsamp\times\nsamp}$ such that $\transempK := U\empK$ is a matrix with at least one row vector of nonzeros; this can be achieved by having an elementary matrix that adds rows together. By the shadedness assumption, such a matrix exists. We can write this operation as 
\begin{align*}
 U\empK = 
 \begin{bmatrix}
  \transempK_{11} & \transempK_{12} & \cdots & \transempK_{1\ncent}\\
  \transempK_{21} & \transempK_{22} & \cdots & \transempK_{2\ncent}\\
  \vdots & \vdots & \ddots & \vdots\\
  \transempK_{\nsamp1} & \transempK_{\nsamp2} & \cdots & \transempK_{\nsamp\ncent}.
 \end{bmatrix}
\end{align*}
Without loss of generality, and abusing notation slightly, let this multiplication lead to one nonzero row, with the rest of the elements of the matrix being zero, as 
\begin{align*}
 U\empK = 
 \begin{bmatrix}
  \kernel_{11} & \kernel_{12} & \cdots & \kernel_{1\ncent}\\
  0 & 0 & \cdots & 0\\
  \vdots & \vdots & \ddots & \vdots\\
  0 & 0 & \cdots & 0
 \end{bmatrix}.
\end{align*}
Since elementary matrices are full-rank, we then have that $\Rank(U\empK) = \Rank(\empK)$. 

To analyze the rank of $\Obs_{\Tset}$, we apply these elementary matrices to every $\empK\in\catempK$. To do so, consider the block-diagonal matrix $\bdU\in\R^{\nsamp\otime\times\nsamp\otime}$ with $U\in\R^{\nsamp\times\nsamp}$ along the diagonal, and zeros everywhere else,
i.e.
\begin{align}
 \bdU:= \begin{bmatrix}
  U   & \zerosU & \cdots & \zerosU\\
  \zerosU & U   & \cdots & \zerosU\\
  \vdots  & \vdots  & \ddots & \vdots\\
  \zerosU & \zerosU & \cdots & U
 \end{bmatrix}.
\end{align}
It can be shown that  $\bdU$ is full-rank, i.e. has rank $\nsamp\otime$. Going back to the observability matrix, we have that 
\begin{align*}
 \bdU\Obs_{\Tset} &= \bdU\catempK\catdualopC\\
%  \underbrace{
%  \begin{bmatrix}
%   U   & \zerosU & \cdots & \zerosU\\
%   \zerosU & U   & \cdots & \zerosU\\
%   \vdots  & \vdots  & \ddots & \vdots\\
%   \zerosU & \zerosU & \cdots & U
%  \end{bmatrix}}_{\bdU\in\R^{\nsamp\otime\times\nsamp\otime}}
%  \underbrace{
%  \begin{bmatrix}
%   \empK   & \zerosK & \cdots & \zerosK\\
%   \zerosK & \empK   & \cdots & \zerosK\\
%   \vdots  & \vdots  & \ddots & \vdots\\
%   \zerosK & \zerosK & \cdots & \empK
%  \end{bmatrix}}_{\catempK\in\R^{\nsamp\otime\times\ncent\otime}}
%  \underbrace{
%  \begin{bmatrix}
%   \JorLa_1^{\tindex_1} & 0 & \cdots & 0\\
%   0 & \JorLa_2^{\tindex_1} & \cdots & 0\\
%    \vdots & \vdots & \ddots & \vdots\\
%   0 & 0 & \cdots & \JorLa_{\JorMul}^{\tindex_1}\\ 
%    \hline
%    \vdots & \vdots & \ddots & \vdots\\
%    \hline
%   \JorLa_1^{\tindex_{\otime}} & 0 & \cdots & 0\\
%   0 & \JorLa_2^{\tindex_\otime} & \cdots & 0\\
%    \vdots & \vdots & \ddots & \vdots\\
%   0 & 0 & \cdots & \JorLa_{\JorMul}^{\tindex_\otime}   
%  \end{bmatrix}}_{\catdualopC\in\R^{\ncent\otime\times\ncent}}\\
 &=  
  \underbrace{
  \begin{bmatrix}
  U\empK   & \zerosK & \cdots & \zerosK\\
  \zerosK & U\empK   & \cdots & \zerosK\\
  \vdots  & \vdots  & \ddots & \vdots\\
  \zerosK & \zerosK & \cdots & U\empK
 \end{bmatrix}}_{\bdU\catempK\in\R^{\nsamp\otime\times\ncent\otime}}
 \underbrace{\catdualopC}_{\in\R^{\ncent\otime\times\ncent}},
\end{align*}
since $\zerosU\zerosK = \zerosK$. Due to the fact that $\Rank(\bdU\Obs_{\Tset}) = \Rank(\Obs_{\Tset})$,
we can therefore perform our rank analysis on the simpler matrix $\Rank(\bdU\Obs_{\Tset})$. Note that
\begin{align*}
 U\empK \dualopApprox^{\tindex_j} &=  
 \begin{bmatrix}
  \kernel_{11} & \kernel_{12} & \cdots & \kernel_{1\ncent}\\
  0 & 0 & \cdots & 0\\
  \vdots & \vdots & \ddots & \vdots\\
  0 & 0 & \cdots & 0
 \end{bmatrix}
 \dualopApprox^{\tindex_j}\\
 &= 
 \begin{bmatrix}
  k_{11}\la_1^{\tindex_j} & \binom{\tindex_j}{1}\la_1^{\tindex_j-1} + k_{12}\la_1^{\tindex_j}  & \cdots & k_{1\ncent}\la_{\JorMul}^{\tindex_j}\\
  0 & 0 & \cdots & 0\\
  \vdots & \vdots & \ddots & 0\\
  0 & 0 & \cdots & 0
 \end{bmatrix}.  
\end{align*}
Therefore, following some more elementary row operations encoded by $V\in\R^{\ncent\otime\times\ncent\otime}$, we have that
\begin{align*}
 V \bdU\Obs_{\Tset}
   &= 
   \begin{bmatrix}
    \kernel_{11}\la_1^{\tindex_1} & \cdots & \kernel_{1\ncent}\la_{\JorMul}^{\tindex_1}\\
    \kernel_{11}\la_1^{\tindex_2} & \cdots & \kernel_{1\ncent}\la_{\JorMul}^{\tindex_2}\\
    \vdots & \ddots & 0\\
    \kernel_{11}\la_1^{\tindex_{\otime}} & \cdots 
    & \kernel_{1\ncent}\la_{\JorMul}^{\tindex_{\otime}}\\
    \zerosMat{\ncent(\otime-1)}{1} & \cdots & \zerosMat{\ncent(\otime-1)}{1}
   \end{bmatrix}\\
   &= 
   \begin{bmatrix}
    \boldsymbol{\Phi}\\
    \zerosMat{\ncent(\otime-1)}{\ncent}
   \end{bmatrix}.
\end{align*}
If the individual entries $\kernel_{1i}$ are nonzero, and the Jordan block diagonals have nonzero eigenvalues, the columns of $\boldsymbol\Phi$
become linearly independent. Therefore, if $\otime \geq \ncent$, the column rank of $\Obs_{\Tset}$ is $\ncent$, which results in an observable system.

To extend this proof to matrices $\dualopApprox = \JorP\JorLa\JorP^{-1}$, note that 
\begin{align*}
 \Obs_{\Tset} &= 
 \begin{bmatrix}
  \empK \dualopApprox^{\tindex_1}\\
  \cdots\\
  \empK \dualopApprox^{\tindex_\otime}
 \end{bmatrix}\\
 &=
  \begin{bmatrix}
  \empK \JorP\JorLa^{\tindex_1}\JorP^{-1}\\
  \cdots\\
  \empK \JorP\JorLa^{\tindex_\otime}\JorP^{-1}.
 \end{bmatrix}\\
 &=
 \catempK
 \catJorP
 \catJorLa^t
 \catJorP^{-1},
\end{align*} 
where $\catJorP\in\R^{\ncent\otime\times\ncent\otime}$, $\catJorLa^t\in\R^{\ncent\otime\times\ncent\otime}$, and
$\boldsymbol{\JorP^{-1}}\in\R^{\ncent\otime\times\ncent\otime}$ are the block diagonal matrices associated with the system. 
Since $\catJorP$ is an invertible matrix, the conclusions about the column rank drawn before still hold, and the system is observable. 
\end{proof}


% When the eigenvalues of the system matrix are repeated, it is not enough for $\empK$ to be shaded.
% The next proposition proves a lower bound on the number of sampling locations required. 
% \begin{proposition}\label{prop:2}
%  Suppose that the conditions in Proposition \ref{prop:1} hold, with the relaxation that
%  the Jordan blocks $\left[\begin{smallmatrix}
%                            \JorLa_1 &\cdots & \JorLa_{\JorMul}
%                           \end{smallmatrix}\right]$ may have 
%  repeated eigenvalues (i.e. $\exists \JorLa_i$ and $\JorLa_j$ s.t. $\eval_i = \eval_j$).
% %  Let $\nevals$ be the number of unique eigenvalues of $A$. 
%  Then there exist kernels $\kernel(x,y)$ such that 
%  the lower bound $\minmeas$ on the number of sampling locations $\nsamp$ is given by the cyclic index of $\dualopApprox$.
% \end{proposition}
\begin{proof}
\textbf{(Proposition \ref{prop:2})}
\textbf{By Contrapositive.} We will show that if the number of sampling locations are $ \nsamp=\ell-1 $ (i.e. $ \nsamp < \ell$), then the system is not observable. Pick the Gaussian kernel in the dictionary of atoms framework,
with sampling locations $x_i\in\sampSet$ and centers $c_j\in\shCent$, with the additional 
property that $x_i\neq x_j \forall i,j\in\{1,\dots,\nsamp\}, i\neq j$.
In this case, 
$\empKShadFull$ has $\minmeas-1$ nonzero, linearly independent rows, and can be written as
\begin{align*}
\empKShadFull &= \begin{bmatrix}
k_{11} & k_{12} & \cdots & k_{1\ncent} \\
\vdots & \vdots & \cdots & \vdots \\
k_{(\minmeas-1)1} & k_{(\minmeas-1)2} & \cdots & k_{(\minmeas-1)\ncent} 
\end{bmatrix}.
\end{align*}
Since the cyclic index is $\minmeas$, this implies that at least one eigenvalue, say $\eval$, has $\minmeas$ Jordan 
blocks. 
	%  For concreteness, suppose $\minmeas=2$, and let the size of the blocks be $i\times i$ and $j\times j$. 
	Define indices $j_1, j_2, \dots, j_{\minmeas} \in \{1,2,\dots,\ncent\}$ as the columns corresponding to the leading entries of the $\minmeas$ Jordan blocks corresponding to $\eval$. WLOG, let $j_1 = 1$.
	Using ideas similar to the last proof, we can write the observability matrix as
	\begin{align*}
	\Obs_{\Tset}
	&:= 
	\begin{bmatrix}
	\kernel_{11}\eval^{\tindex_1}  & \cdots & \kernel_{1j_{\minmeas}}\eval^{\tindex_1} & \cdots\\
	\vdots & \ddots &\vdots & \ddots\\
	\kernel_{11}\eval^{\tindex_{\otime}}  & \kernel_{1j_{\minmeas}}\eval^{\tindex_{\otime}} & \cdots\\
	\vdots & \ddots & \vdots & \ddots\\
	\kernel_{(\minmeas-1)1}\eval^{\tindex_1}  \cdots & \kernel_{(\minmeas-1)j_{\minmeas}}\eval^{\tindex_1} & \cdots\\
	\vdots & \ddots &\vdots & \ddots\\
	\kernel_{(\minmeas-1)1}\eval^{\tindex_{\otime}}  & \cdots & \kernel_{(\minmeas-1)j_{\minmeas}}\eval^{\tindex_{\otime}} & \cdots
	\end{bmatrix}.
	\end{align*}
	Define $\evalvec:= \begin{bmatrix}\eval^{\tindex_1} & \eval^{\tindex_2} & \cdots \eval^{\tindex_{\otime}}\end{bmatrix}^T$. 
	Then the above matrix becomes 
	\begin{align*}
	\Obs_{\Tset}
	&:= 
	\begin{bmatrix}
	\kernel_{11}\evalvec  & \cdots & \kernel_{1j_2}\evalvec & \cdots & \kernel_{1j_{\minmeas}}\evalvec & \cdots\\
	\vdots & \ddots & \vdots & \ddots &\vdots & \ddots\\
	\kernel_{(\minmeas-1)1}\evalvec  & \cdots & \kernel_{(\minmeas-1)j_2}\evalvec & \cdots & \kernel_{(\minmeas-1)j_{\minmeas}}\evalvec & \cdots
	\end{bmatrix}.
	\end{align*}
	We need to show that one of the columns above can be written in terms of the others. This is equivalent to solving the linear system
	\begin{align*}
	\begin{bmatrix}
	\kernel_{1j_1}\\
	\kernel_{2j_1}\\
	\vdots\\
	\kernel_{(\minmeas-1)j_1}
	\end{bmatrix}
	%   &=
	%   c_1
	%   \begin{bmatrix}
	%    \kernel_{1j_2}\\
	%    \kernel_{2j_2}\\
	%    \vdots\\
	%    \kernel_{(\minmeas-1)j_2}
	%   \end{bmatrix} 
	%   + \cdots + c_{\minmeas-1}
	%   \begin{bmatrix}
	%    \kernel_{1j_{\minmeas}}\\
	%    \kernel_{2j_{\minmeas}}\\
	%    \vdots\\
	%    \kernel_{(\minmeas-1)j_{\minmeas}}
	%   \end{bmatrix}\\
	&=
	\begin{bmatrix}
	\kernel_{1j_2} & \cdots & \kernel_{1j_{\minmeas}}\\
	\kernel_{2j_2} & \cdots & \kernel_{2j_{\minmeas}}\\
	\vdots & \ddots & \vdots\\
	\kernel_{(\minmeas-1)j_2} & \cdots & \kernel_{(\minmeas-1)j_{\minmeas}}\\
	\end{bmatrix} 
	\begin{bmatrix}
	c_1\\
	c_2\\
	\vdots\\   
	c_{(\minmeas-1)}
	\end{bmatrix}. 
	\end{align*}
	Since the kernel matrix on the RHS is generated from the Gaussian kernel, from \cite{micchelli1984interpolation}, 
	it's known that every principal minor of a Gaussian kernel matrix is invertible, which implies that $\Obs_{\Tset}$ cannot be observable. 
\end{proof}


%Recall that $\minmeas > 1$ implies that there exist spaces $\linspace_i$ s.t. $\R^{\ncent} = \linspace_1\oplus\cdots\oplus\linspace_{\minmeas}$, which induces the decomposition $\fspaceApprox = \fsubspaceC{1}\oplus\cdots\oplus\fsubspaceC{\minmeas}$. 
%% This implies that an initial condition $\initcondf\in\fsubspaceC{\iota}$, remains in $\fsubspaceC{\iota}$ under the action of $\sysopC$, even if the measurement operator $\measop$ corresponds to a shaded matrix $\empK$. 
%As the simplest nontrivial case, consider $\shCent=\{c_1,c_2\}$, $c_i\in\dom$, and pick one sensor location $x_1\in\dom$. Let \eqref{k_measure} be given by 
%$\dualopApprox=\left[\begin{smallmatrix}\eval & 0\\ 0 &\eval\end{smallmatrix}\right]$, $\eval \in\R$ and $ \eval < 0$, and let $\processnoise_{\tindex}$ and $\measnoise_{\tindex}$ be zero. Here, $\minmeas=2$, because there exists no $\linvec\in\R^2$ s.t. $\Span\{\linvec,\dualopApprox\linvec\} = \R^2$. Any initial condition $\initcond$ is an eigenvector for $\dualopApprox$, and thus we get a discrete sequence $\{\initcond, \eval\initcond, \eval^2\initcond,\dots\}$ going to zero along the 1-dimensional subspace generated by $\initcond$. Let $\Tset = \{0, 1\}$ and consider a shaded matrix $\empK = \left[\begin{smallmatrix}\kernel_{11} & \kernel_{12}\end{smallmatrix}\right]$: then the observability matrix is given by $\Obs_{\Tset} = \left[\begin{smallmatrix}\empK^T & (\empK\dualopApprox)^T \end{smallmatrix}\right]^T = \left[\begin{smallmatrix}\kernel_{11} &
%\kernel_{12} \\ \eval\kernel_{11} & \eval\kernel_{12} \end{smallmatrix}\right] = 
%\left[\begin{smallmatrix}\kernelvec_{1}^T &
%\\ \eval\kernelvec_{1}^T\end{smallmatrix}\right]$, which is obviously rank-deficient, and where
%$\left[\begin{smallmatrix}
%\kernel_{11} & \kernel_{12}
%\end{smallmatrix}\right]^T:= \kernelvec_1$. More intuitively, we have that $\Obs_{\Tset}\initcond = 
%\left[\begin{smallmatrix}\l\kernelvec_1, \initcond \r_{\R^2} \\
%\eval\l\kernelvec_1, \initcond \r_{\R^2}\end{smallmatrix}\right]$, which implies that $\kernelvec_1$ doesn't have enough geometric information to recover the initial state. Contrast this with the case when $\dualopApprox=\left[\begin{smallmatrix}\eval & 1\\ 0 &\eval\end{smallmatrix}\right]$; here, $\minmeas=1$, and
%$\Obs_{\Tset} = \left[\begin{smallmatrix}\kernel_{11} &
%\kernel_{12} \\ \eval\kernel_{11} & \kernel_{11} + \eval\kernel_{12} \end{smallmatrix}\right]$, which is full rank for a shaded kernel matrix $\empK$.

% Section \ref{sec:discussion} gives a concrete example to build intuition regarding this lower bound. 
% We now show how to construct a matrix $\measmap$ corresponding to the lower bound $\minmeas$.
% \begin{proposition}
% 	Given the conditions stated in Proposition \ref{prop:2}, it is possible to construct a measurement map $\measmap \in \R^{\minmeas\times\ncent}$ for the system given by \eqref{k_measure}, such that the pair $(\measmap, \dualopApprox)$ is observable.
% \end{proposition}
\begin{proof}
\textbf{(Proposition \ref{prop:3})}
The construction of the measurement map $\measmap$ is based on the rational canonical structure of $\dualopApprox^T$, which decomposes $ \linspace $ into $\dualopApprox^T$-cyclic direct summands such that $\linspace = \linspace_1 \oplus \cdots \oplus \linspace_\minmeas$, where $\minmeas$ is the cyclic index of $\dualopApprox$. Let $\minpolyv_{\linvec}$ be the minimal polynomial (m.p.) of $ \linvec $ (relative to $\dualopApprox^T$): it is then the unique monic polynomial of least degree such that $\minpolyv_{\linvec}(\dualopApprox^T)\linvec=0$. Let $\minpoly_1(\eval)$ be the m.p. of ${\dualopApprox^T}_{|\linspace_1}$: then $\degs(\minpoly_1(\eval)) < \ncent$. By the rational canonical structure theorem \cite{wonham1974linear}, there exists a vector $\widehat{\linvec}_1$, such that $\minpolyv_{\linvec_1}(\eval)=\minpoly_1(\eval)$. Similarly there exists a vector $\widehat{\linvec}_2$, such that $\minpolyv_{\linvec_2}(\eval)=\minpoly_2(\eval)$, where $\minpoly_2(\eval)$, is the minimal polynomial of ${\dualopApprox^T}_{|\linspace_2}$ and so on. Thus we can obtain $\minmeas$ such vectors that form the measurement map $\measmap = [\widehat{\linvec}_1, \widehat{\linvec}_2,\cdots, \widehat{\linvec}_\minmeas]^T$. Construction of these vectors $\widehat{\linvec}_i$, can be simplified by first performing the Jordan decomposition as $ \dualopApprox^T = \JorP\JorLa\JorP^{-1} $. Then the vectors $ \widetilde{\linvec_i},\ i\in \{1,\dots,\minmeas\}$ for $ \JorLa $, can be constructed such that the entries corresponding to the leading entries of Jordan blocks of $ \JorLa_{|\linspace_i} $ are nonzero. Such a construction ensures that the m.p. of vector $ \widetilde{\linvec_i}$ w.r.t $\JorLa_{|\linspace_i}$, is also the corresponding m.p. of 
$\JorLa_{|\linspace_i}$. Finally, the required map can be obtained as $ \measmap = [\widetilde{\linvec_1}, \widetilde{\linvec_2},\dots,\widetilde{\linvec_\minmeas}]^T\JorP^{-1}$.
\end{proof}
%                                                                                                                                                                                    
% 
% The construction provided in the proof of Proposition \ref{prop:3} is utilized in Algorithm \ref{alg:measmap}, which uses the rational canonical structure of $\dualopApprox$ to generate a series of vectors $\linvec_i\in\R^{\ncent}$, whose iterations $\{\linvec_1,\dots,\dualopApprox^{\acycdeg_1-1}\linvec_1,\dots,\linvec_{\minmeas},\dots,\dualopApprox^{\acycdeg_{\minmeas}-1}\linvec_{\minmeas}\}$ generate a basis for $\R^{\ncent}$ (see Section \ref{sec_prelim}). 
% Unfortunately, the measurement map $\measmap$, being an abstract construction unrelated to the kernel, does not directly select $\sampSet$. We will show how to use the measurement map to guide a search for $\sampSet$ in Remark \ref{rem:1}. For now, we state a sufficient condition for observability of a general system. 
% \begin{theorem}\label{thm:1}
% 	Suppose that the conditions in Proposition \ref{prop:1} hold, with the relaxation that
% 	the Jordan blocks $\begin{bmatrix}\JorLa_1 & \JorLa_2 &\cdots & \JorLa_{\JorMul}\end{bmatrix}$ may have 
% 	repeated eigenvalues. Let $\minmeas$ be the cyclic index of $\dualopApprox$.
% 	We define 
% 	\begin{align}\eqlabel{empKShadFull}
% 	\empKShadFull = \left[\begin{smallmatrix}
% 	\empK^{{(1)}^T} & 
% 	\cdots &
% 	\empK^{{(\minmeas)}^T}
% 	\end{smallmatrix}\right]^T
% 	\end{align}
% 	as the \emph{$\minmeas$-shaded matrix} which consists of $\minmeas$ shaded matrices with the property that any subset of
% 	$\minmeas$
% 	columns in the matrix are linearly independent from each
% 	other. Then system \eqref{k_measure} is observable if $\Tset$ has distinct values, and $|\Tset| \geq \ncent$.
% \end{theorem}
\begin{proof}
\textbf{(Theorem \ref{thm:1})}
A cyclic index of $\minmeas$ for this system implies that there exists an eigenvalue $\eval$ that's repeated $\minmeas$
times. We prove the theorem for repeated eigenvalues of dimension 1: the same statement can be proven for repeated eigenvalues for Jordan blocks using the ideas in the proof of Proposition \ref{prop:1}. 
WLOG, let $\empKShadFull$ have $\minmeas$ fully shaded, linearly independent rows, and, assume that the column indices corresponding to this eigenvalue are $\{1,2,\dots,\minmeas\}$. Define 
$\evalvec_i:= \begin{bmatrix}\eval_i^{\tindex_1} & \eval_i^{\tindex_2} & \cdots \eval_i^{\tindex_{\otime}}\end{bmatrix}^T$. Then
	\begin{align*}
	\Obs_{\Tset}
	&:= 
	\begin{bmatrix}
	k_{11} \evalvec_1 & k_{12} \evalvec_2 & \cdots & k_{1\ncent} \evalvec_{\ncent}\\
	\vdots & \vdots & \ddots & \vdots\\
	k_{\minmeas 1} \evalvec_1 & k_{\minmeas 2} \evalvec_2 & \cdots & k_{\minmeas \ncent} \evalvec_{\ncent}
	\end{bmatrix}.
	\end{align*}
	Let $\evalvec_1 = \evalvec_2 = \cdots \evalvec_{\minmeas} := \evalvec$. 
	Focusing on these first $\minmeas$ columns of this matrix, this implies that
	we need to find constants $c_1,c_2,\dots, c_{\minmeas-1}$ such that
	\begin{align*}
	\begin{bmatrix}
	k_{11}\\
	\vdots\\
	k_{\minmeas 1}
	\end{bmatrix}
	&= 
	c_1 
	\begin{bmatrix}
	k_{12}\\
	\vdots\\
	k_{\minmeas 2}
	\end{bmatrix}
	+ \cdots + 
	c_{\minmeas-1} 
	\begin{bmatrix}
	k_{1\minmeas}\\   
	\vdots\\
	k_{\minmeas \minmeas} 
	\end{bmatrix}.
	\end{align*}
	However, these columns are linearly independent by assumption, and thus no such constants exist, implying that $\Obs_{\Tset}$ is observable. 
\end{proof}
While Theorem \ref{thm:1} is a quite general result, the condition that any $\minmeas$ columns of $\empKShadFull$ be linearly independent is a very stringent condition. 
One scenario where this condition can be met with minimal measurements is in the case when the feature map $\fmapApprox(x)$ is generated by a dictionary of atoms with the Gaussian RBF kernel evaluated at sampling locations $\sampSetLong$ according to \eqref{fmap_dict}, where $x_i\in\dom\subset\R^d$, and $x_i$ are sampled from a non-degenerate probability distribution on $\dom$ such as the uniform distribution. For a semi-deterministic approach, when the dynamics matrix $\dualopApprox$ is block-diagonal, we can utilize a simple heuristic:
\begin{remark}\label{rem:1}
	Let $\dom$ be compact, $\shCent = \shCentLong$, $c_i\in\dom$, and let the approximate feature map be defined by \eqref{fmap_dict}. Consider the system \eqref{k_measure} with $\dualopApprox=\JorLa$, and let $\Tset = \{0,1,\dots,\ncent-1\}$. Then the measurement map $\measmap$'s values lie in $\{0, 1\}$; in particular, each row $\measmap^{(j)}$, $j\in\{1,\dots,\minmeas\}$, corresponds to a subspace $\fsubspaceApprox{j}$, generated by a subset of centers $\shCent^{(j)}\subset\shCent$. Generate samples $x_i^{(j)}$ to create a kernel matrix $\empK^{(j)}$ that is shaded only with respect to centers $\shCent^{(j)}$. Once this is done, move on to the next subspace $\fsubspaceApprox{j+1}$. When all $\minmeas$ rows of $\measmap$ are accounted for, construct the matrix $\empKShadFull$ as in \eqref{empKShadFull}. Then the resulting system $(\empKShadFull, \dualopApprox)$ is observable. 
\end{remark}
This heuristic is formalized in Algorithm 2. Note that in practice, the matrix $\dualopApprox$ needs to be inferred from measurements of the process $f_{\tindex}$. If no assumptions are placed on $\dualopApprox$, it's clear that at least $\ncent$ sensors are required for the system identification phase. Future work will study the precise conditions under which system identification is possible with less than $\ncent$ sensors.
\begin{figure}[t!]
	\begin{algorithm}[H]
		\caption{Measurement Map $\measmap$}
		\label{alg:measmap}
		\begin{algorithmic}
			\begin{footnotesize} 
				\STATE {\bfseries Input:} $\dualopApprox\in\R^{\ncent\times\ncent}$
				\STATE Compute Frobenius canonical form, such that $ \FrobC  = \FrobP^{-1} \dualopApprox^T\FrobP$. Set $\FrobC_0:=\FrobC$, and
				$\ncent_0:=\ncent$. 
				%           Obtain subspaces 
				%           $\linspace_i\subset\R^{\ncent}$ s.t. $\R^{\ncent} = \linspace_1\oplus\cdots\oplus\linspace_{\minmeas}$.
				\FOR{$i=1$ {\bfseries to} $\minmeas$}
				\STATE Obtain MP $\minpoly_i(\eval)$ of $\FrobC_{i-1}$. 
				This returns associated indices $\mmapInd{i}\subset\{1,2,\dots,\ncent_{i-1}\}$. 
				%             \STATE Use $\mmapInd{i}$ to select submatrix $\widetilde{\FrobP}_i$, where an index $j\in\mmapInd{i}$ 
				%                    indicates selection of the row and column associated to $j$.
				\STATE Construct vector $\linvec_i \in \R^{\ncent}$ such that 
				$\minpolyv_{\linvec_i}(\eval)=\minpoly_{i}(\eval)$ .
				\STATE Use indices $\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}$ to select matrix $\FrobC_i$. Set 
				$\ncent_i:= |\{1,2,\dots,\ncent_{i-1}\}\setminus\mmapInd{i}|$
				\ENDFOR
				\STATE Compute $ \premeasmap = [\linvec_1^T, \linvec_2^T,...,\linvec_{\minmeas}^T]^T$
				\STATE {\bfseries Output:} $ \measmap =\premeasmap\FrobP^{-1}  $
			\end{footnotesize}
		\end{algorithmic}
	\end{algorithm} 
\end{figure}
\begin{figure}[t!]
	\begin{algorithm}[H]
		\caption{Sampling locations set $ \sampSet $}
		\label{alg:samples}
		\begin{algorithmic}
			\begin{footnotesize}	
				\STATE {\bfseries Input:} $ \dualopApprox =\FrobC $, lower bound $\minmeas$
				%		\STATE Check $k=\max_i \nu(\lambda_i)$
				\STATE Decompose $ \FrobC $ to generate invariant subspaces $ \fsubspaceC{j} $, $ j\in \{1,2,\dots,\minmeas\} $ (see section \ref{sec_prelim})
				\FOR{$j=1$ {\bfseries to} $\minmeas$}
				\STATE Obtain centers $\shCent^{(j)}$ w.r.t subspace $ \fsubspaceC{j} $,
				\STATE Generate samples $x_i^{(j)}$ to create a kernel matrix $\empK^{(j)}$ that is shaded only with respect to centers $\shCent^{(j)}$
				\ENDFOR
				\STATE {\bfseries Output:} Sampling locations set  $\sampSet =\{x^{(1)}, x^{(2)}\cdots,x^{(l)}\} $.
			\end{footnotesize}		
		\end{algorithmic}
	\end{algorithm}
\end{figure}
\begin{proof}
	\textbf{(Theorem \ref{thm:r1})}
	For each random sample, the probability that it lies within the $ \epsilon- $shaded region of a particular center $ c_j \in \shCent_\rands$
	is at least $ p_{\e} $. The series of random samples can be considered as Bernoulli trials in which $p_{\e}$ is the probability of a successful outcome. Note this is assuming worst case scenario that the intersection between any two $ \epsilon $-shaded region of centers belonging to the set  $ \shCent_\rands $ is empty. Observability for the pair $ (\empK, \dualopApprox) $ is achieved after $ \rands  $ successful outcomes are obtained because each success ensures a row vector with non-zero entry corresponding to the leading entry of the Jordan block. 
	
	Let $ X_1, X_2, \dots, X_\nsamp $ be i.i.d. random variables whose common distribution is the Bernoulli distribution with parameter $p_{\e}$. The random variable $ X = X_1 + X_2+ \dots+ X_\nsamp $ denotes the number of success after $ \nsamp $ random samples. Since each $ X_i $ has the Bernoulli distribution, $ X $ will have binomial distribution, 
	\begin{equation*}
	P(X=h) = {\nsamp \choose h}p_{\e}^h (1-p_{\e})^{\nsamp-h},
	\end{equation*}
	in which $ h  $ is the number of success. The expectation of the binomial distribution, that is the expected number of success is $ Np_{\e} $, and thus the expected number of trials required will be $ N = \rands/p_{\e}$.
\end{proof}
\begin{proof}
	\textbf{(Theorem \ref{thm:r2})}
	The random variable $ X $ from the proof of Theorem 1 has a binomial distribution, which enables the application of a Chernoff-type bound on its tail probabilities. A well known result  on multiplicative Chernoff bound \cite{motwani2010randomized} is directly applied to establish this Theorem. If $ X $ is binomially distributed, $ \delta \in (0,1] $, and $ \meanDist = \mathbb{E}[X] $, then $ P[X\leq(1-\delta)\meanDist] \leq \exp(-\meanDist \delta^2/2) $, in which we let $ \delta = 1-\frac{\rands}{\nsamp p_{\e}}$.  The expression in the exponent can be simplified to $ -\frac{1}{2}\nsamp p_{\e}+\rands - \frac{\rands^2}{2\nsamp p_{\e}} $, using $ \meanDist = \nsamp p_{\e} $. Note that $ e^{\frac{-\rands^2}{2\nsamp p_{\e}}} \leq 1$. This implies that,
	\begin{equation*}
	\exp(-\meanDist \delta^2/2) =  e^{-\frac{1}{2}(\nsamp p_{\e}-2\rands)}.e^{\frac{-\rands^2}{2\nsamp p_{\e}}}\leq e^{-\frac{1}{2}(\nsamp p_{\e}-2\rands)}
	\end{equation*}
	Note, $ (1-\delta)\mu = \rands $, hence we obtain that $ P[X\leq\rands] \leq e^{-\frac{1}{2}(\nsamp p_{\e}-2\rands)}. $
\end{proof}
 \begin{figure}[t!]
 	\centering
 	\begin{algorithm}[H]
 		\caption{Kernel Observer (Transition Learning)}
 		\label{alg:egp_trans}
 		\begin{algorithmic}
 			\begin{footnotesize}
 				\STATE {\bfseries Input:} Kernel $\kernel$, basis centers $\shCent$, final time 
 				step $\ftime$. 
 				\WHILE{$\tindex \leq \ftime$}
 				\STATE $1)$ Sample data $\{y^i_{\tindex}\}_{i=1}^{\ncent}$ from $f_{\tindex}$. 
 				\STATE $2)$ Estimate $\estweight_\tindex$ via standard kernel inference procedure. 
 				\STATE $3)$ Store weights $\estweight_\tindex$ in matrix $\W\in\R^{\ncent\times \ftime}$.
 				\ENDWHILE
 				\STATE To infer $\dualopApprox$, define matrix $\Phi = \W^T\W$. Then:
 				\FOR{$i=1$ {\bfseries to} $\ncent$}
 				\STATE At step $i$, solve system
 				\begin{align}
 				\dualopApprox^{(i)} = \left(\left(\Phi + \la I\right)^{-1}(\W^T\W^{(i)})\right)^T,
 				\end{align}
 				where $\dualopApprox^{(i)}$, and $\W^{(i)}$ are the $i$th columns of $\dualopApprox$ and $\W^{(i)}$ respectively. 
 				\ENDFOR
 				\STATE Compute the covariance matrix  $\estcontrolop$ of the observed 
 				weights $\W$. 
 				\STATE {\bfseries Output:} estimated transition matrix $\dualopApprox$, predictive covariance    
 				matrix $\estcontrolop$. 
 			\end{footnotesize}
 		\end{algorithmic}
 	\end{algorithm}
 	\vspace{-0.3in}
 \end{figure}
 \begin{figure}[t!]
 	\begin{algorithm}[H]
 		\caption{Kernel Observer (Monitoring and Prediction)}
 		\label{alg:egp_inf}
 		\begin{algorithmic}
 			\begin{footnotesize}
 				\STATE {\bfseries Input:} Kernel $\kernel$, basis centers $\shCent$, 
 				estimated system matrix $\dualopApprox$, estimated covariance matrix $\estcontrolop$.
 				\STATE {\bfseries Compute Observation Matrix:} Compute the cyclic index $\minmeas$ of $\dualopApprox$, and compute $\empK$.
 				\STATE {\bfseries Initialize Observer:} Use $\dualopApprox$, $\estcontrolop$, and $\empK$ to initialize a state-observer (e.g. Kalman filter (KF)) on $\fspaceApprox$.
 				\WHILE{ measurements available }   
 				\STATE 1) Sample data $\{y^i_{\tindex}\}_{i=1}^{\nsamp}$ from $f_{\tindex}$.
 				\STATE 2) Propagate KF estimate $\estweight_{\tindex}$ 
 				forward to time $\tindex+1$, correct using measurement feedback with $\{y^i_{\tindex+1}\}_{i=1}^{\nsamp}$. 
 				\STATE 3) Output predicted function $\widehat{f}_{\tindex+1}$ of KF.
 				\ENDWHILE   
 			\end{footnotesize}
 		\end{algorithmic}
 	\end{algorithm}
 	\vspace{-0.2in}
 \end{figure}
\begin{figure}[tbh]
 \begin{algorithm}[H]
   \caption{Kernel Controller}
   \label{alg:egp_control}
\begin{algorithmic}
\begin{footnotesize}
   \STATE {\bfseries Input:} Kernel $\kernel$, basis points $\shCent$,
   estimated system matrix $\estsysop$, estimated covariance matrix $\estcontrolop$, and function $f_{\text{ref}}$ 
   to drive initial function to.    
   \STATE {\bfseries Initialize Observer:} (see Algorithm \ref{alg:egp_inf}). 
   \STATE {\bfseries Initialize Controller:} Use Jordan decomposition of $\estsysop$ to obtain  no. of control locations $\controlCent$, compute kernel matrix $\empKCD\in\R^{\ncontrol\times\ncent}$ between $\controlCent$ and $\shCent$, and initialize controller (e.g. LQR) utilizing $(\estsysop, \estcontrolop)$.
   \WHILE{ measurements available }   
     \STATE 1) Sample data $\{y^i_k\}_{i=1}^{\nsamp}$ from $f(x,\tindex)$.    
     \STATE 2) Utilize observer to estimate $\estweight_{\tindex+1}$.
     \STATE 3) Use $\estweight_{\tindex+1}$ and $f_{\text{ref}}$ as input to controller to get feedback. 
   \ENDWHILE   
\end{footnotesize}
\end{algorithmic}
\end{algorithm}
\vspace{-0.2in}
\end{figure}
